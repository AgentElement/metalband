		raw count							19 (16.1%)	65 (55.08%)	2 (1.69%)	14 (11.86%)	118 (100%)	118	117	81	61 (51.69%)	35 (29.66%)	32 (27.11%)	21 (17.79%)	61 (51.69%)	10 (8.47%)		86	74	68	24	37	50	20	22	54	0	27	#DIV/0!	#DIV/0!	#DIV/0!	#DIV/0!	6 (5.08%)	11 (9.32%)	30 (25.42%)	30	28 (23.72%)	24 (20.33%)	22	26	27	#DIV/0!	11		#DIV/0!	68	18	58	#DIV/0!	#DIV/0!	#DIV/0!		
												# of Initial papers:13		Joe:50	Pemma:68	ML keyword or technique: 34.8%	APR + ML Technique: 22; APR and No ML Technique: 39; APR + LLM: 7; APR and No LLM: 54	39	# of ML papers:32 (27.11%)							AVAILABILITY				RESOURCES			Reproducibility							7	Architecture							Dataset			Fault Localization		Input		Patch evaluations				
Conference	Authors	Title	year	link	original order	MLA cite	keywords	Conf keywords	Workshop or some other paper that is not fully fleshed out (phd proposal, journal-first)  or Non-adjacent to APR	In Monperrus Full?	In Search Terms / Monperrus Results?	In initial sample?	reviewed?	Reviewer	Summary	Notes	APR tool?	Has ML keyword?	Uses ML technique?	Paper focuses on Patch Correctness or Patch Quality?	Did it evaluate on a benchmark?	Were prior evaluation results used? [no new APR evaluations performed]	Defects4j	Which datasets were used in evaluation?	New Model / Tool / Tool-Augment / Benchmark Name	CODE	TRAINED MODEL	DATASET	replication package?	IF Reported, Hours Training / Running 	Compute Resources used for Training / Running	efficiency - upper limit (only in methods), average time taken in eval, or average test suite evals in eval.	Reproduced in other independent papers / evaluations	what other models / tools were reproduced in this evaluation?	deduplication effort	validation/evaluation dataset	fine-tuning dataset	pre-training/training dataset	uses existing pretrained LM?	LLM?	Uses ML?	ML architecture?	neural network?	Transformer model?	Attention model detailed / explained?	# of parameters?	context / input size? (tokens or lines)	New benchmark introduced?	New benchmark features? (new attribute?)	Which datasets evaluated?	Assumes Perfect Fault Localization?	Fault localization type (Perfect Line, Perfect Function, Context and Context Size; Spectra Type)	Additional required inputs? (ML: outline their tokenization / generalizing method for their input and number of tokens; Program History, etc.)	reports summary info or per-bug info	Plausible reported?	Correct reported?	"correct" definition	Search termination	beam search info and impact
ICSE	Ren, Zhilei, et al	Automated patching for unreproducible builds.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9793866	1	Ren, Zhilei, et al. "Automated patching for unreproducible builds." Proceedings of the 44th International Conference on Software Engineering. 2022.	reproducible builds; dynamic tracing; automated patch generation; 	Location awareness; Codes; Linux; Computer bugs; Software; Reproducibility of results; Probes;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Software security issues emerge at multiple levels, including at compile time. As a response to malware injection via compilation, "reproducible" builds have been introduced, which provide an audit trail. A build that fails to reproduce is labeled "unreproducible" and this paper aims to fix these builds (i.e., cause them to generate bit-identical binaries).	Build repair more than program repair. And "repair" here is specious - they change build commands so that output is made consistent across a variety of build environments. This might be stripping out things like date and locale information which vary.	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	Custom benchmark of Debian and Arch packages.	RepFix	https://rezilla.bitbucket.io/repfix/		https://rezilla.bitbucket.io/repfix/	https://rezilla.bitbucket.io/repfix/										FALSE	FALSE	FALSE		FALSE	FALSE				FALSE	more a dataset than a benchmark; they made their data available, but it wasn't built for others to use		FALSE				FALSE	FALSE			
ICSE	Meng, Xiangxin, et al	Improving fault localization and program repair with deep semantic features and transferred knowledge.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9794014	2	Meng, Xiangxin, et al. "Improving fault localization and program repair with deep semantic features and transferred knowledge." Proceedings of the 44th International Conference on Software Engineering. 2022.	Fault localization; program repair; transfer learning; neural networks; software debugging;	Location awareness; Codes; Semantics; Computer bugs; Maintenance engineering; Feature extraction; Software debugging;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	This paper presents TRANSFER, an ML enhancement to TBar which uses BiLSTM classifiers for FL and Patch Generation. They fix 6 more bugs than TBar, and propose that this is due to better targeting of location / fix template pairs.		TRUE	TRUE	TRUE	TRUE	TRUE	FALSE	TRUE	Defects4J	TRANSFER	https://github.com/mxx1219/TRANSFER	provided, with detailed instructions for how to run (or how to retrain from scratch if desired)	https://github.com/mxx1219/TRANSFER	https://github.com/mxx1219/TRANSFER	Samples, cross-validation replications and resources reported (but not wallclock time).	We set the running time of each repair process to 3 hours, which is the same as TBar [32]. All the experiments are conducted on Ubuntu 18.04 server with 20 cores of 2.4GHz CPU, 384GB RAM and NVIDIA Tesla V100 GPUs with 32 GB memory.	limit set to 3h (same as TBar)							FALSE	FALSE	TRUE	BiLSTM multi-classifier	TRUE	FALSE	not beyond BiLSTM label	not specified	not specified	FALSE	Main eval was D4J, but they introduce two large datasets for training models for FL and APR.	Defects4J	FALSE	FL is one of the subtasks (handled by their neurlal net). Fault localization is at the statement level, and the model also predicts which fix templates should be selected to generate patches.		per-bug info (in figure 5)	FALSE	TRUE	semantic equivalence to human repair; they also essentially identify "wrong location" as "plausible but incorrect" which is interesting	can't find	
ICSE	Ye, He, Matias Martinez, and Martin Monperrus	Neural program repair with execution-based backpropagation.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9793856	3	Ye, He, Matias Martinez, and Martin Monperrus. "Neural program repair with execution-based backpropagation." Proceedings of the 44th International Conference on Software Engineering. 2022.	automated program repair; software reliability;	Backpropagation; Training; Computer bugs; Semantics; Neural networks; Maintenance engineering; Syntactics;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	This paper introduces RewardRepair, an improvement of NMT-based program repair that introduces semantic (runtime) information to augment purely syntactic analysis. They write a better loss function, essentailly. More of their patches compile than the competition (which makes sense).	considers only single-hunk bugs	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4j 1.2, 2.0, Bugs.jar, QuixBugs	RewardRepair	https://github.com/ASSERT-KTH/RewardRepair	https://doi.org/10.5281/zenodo.5997686	https://github.com/ASSERT-KTH/RewardRepair	https://github.com/ASSERT-KTH/RewardRepair			beam search of 200 (vs. 1k for CURE, CoCoNuT)							FALSE	FALSE	TRUE	Transformer model from Hugging Face	TRUE	TRUE	not directly; reference provided	vocab size 32128	max 512 input tokens from buggy/context code. max 100 tokens in output patch	FALSE		trained on Bears, tested on the four mentioned to the left	FALSE	sbfl w/ Gzoltar, also provides data for perfect fault localization		Per-bug info available in repo but not in paper	FALSE	TRUE	semantic equivalence to human repair (not necessarily identical). consensus of 2 authors if not identical	beam search, sizes = [30, 100, 200, (500 available in supplementary material)]	"our experiment shows a bigger beam size indeed leads to more correct patches generated, which confirms the study of Tufano et al. [ 56 ]." vocabulary size of 32,128 beam sizes=[30, 100, 200, (500)]
ICSE	Gissurarson, Matthías Páll, et al	PropR: property-based automatic program repair.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9794120	4	Gissurarson, Matthías Páll, et al. "PropR: property-based automatic program repair." Proceedings of the 44th International Conference on Software Engineering. 2022.	automatic program repair; search based software engineering; synthesis; property-based testing; typed holes;	Location awareness; Program processors; Redundancy; Manuals; Maintenance engineering; Inspection; Software;	FALSE	TRUE	FALSE	TRUE	TRUE	Joe	Introduces PropR, a program repair tool for Haskell. Claims "property-based tests" are faster and more accurate than unit tests. Less overfitting.	Properties are attributes of functions (e.g. symmetry, idempotency, etc). Easier in functional langs, though apparently tools exist in a few others.	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	Custom one introduced here; based on Haskell course student responses a la IntroClass	PropR	https://github.com/Tritlo/PropR		https://doi.org/10.5281/zenodo.5389051	https://doi.org/10.5281/zenodo.5389051		8 vCPUs @ 3.6ghz 16gb ram	10 mins							FALSE	FALSE	FALSE		FALSE	FALSE				TRUE	IntroClass-inspired thing for Haskell; 30 bugs.		FALSE	SBFL w/ input from property tests as well as unit tests		per bug info in repo	TRUE	TRUE	they manually inspected a subset of their patches; they expect 62/76 to be correct if that ratio holds for the larger set	"we apply the current fixes we have found so far to the targets and enter the next iteration of the loop 10 , repeating the process with the new targets until all properties have been fixed, or the (10 minute) search budget runs out."	
ICSE	Zhao, Yanjie, et al	Towards automatically repairing compatibility issues in published android apps.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9794093	5	Zhao, Yanjie, et al. "Towards automatically repairing compatibility issues in published android apps." Proceedings of the 44th International Conference on Software Engineering. 2022.	Android; Compatibility Issue; Automated Program Repair;	Runtime; Operating systems; Ecosystems; Prototypes; Maintenance engineering; Computer crashes; Internet;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	RepairDroid is APR for android apps (specifically compatibility issues, like crash at runtime on a specific environment). Templates are applied by RepairDroid to fix the issue at bytecode level (right before user installs). Fixes 7660/8976 issues in 1k randomly selected Google Play apps	fault localization done by pattern-matching on decompiled, interemediate representation (Jimple IR), identifies bug statements and methods. They work on published apps, so don't have access to source code. 85.34% success rate at fixing issues in these 1k apps. Pretty impressive	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	AndroZoo	RepairDroid						extremely fast; less than 2 minutes for each of the apps in all cases (typically around 30 seconds)	not specified, but took less than 2 mins per app							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		AndroZoo (1k real apps from Google Play)	FALSE	scans decompiled code at the statement level for patterns matching templates			TRUE	TRUE	they randomly sampled 20 of the apps and tested to ensure that the compatibility issues were indeed fixed; they all were. the other 980 were not executed by them directly (they just looked at whether it crashed which it used to)		
ICSE	Noller, Yannic, et al	Trust enhancement issues in program repair.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9794080	6	Noller, Yannic, et al. "Trust enhancement issues in program repair." Proceedings of the 44th International Conference on Software Engineering. 2022.	program repair;	Semantics; Computer bugs; Maintenance engineering; Software; Human in the loop; Software engineering;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Survey research of practitioners on what they want (for trust and convenience) from APR tools. Bottom line: 10 patches is about the most they're willing to try out, and an hour is the limit on acceptable time. Not much interest in providing inputs while sitting there (so human-in-the-loop is a bad idea).	Presents results of a constrained evaluation on ManyBugs; no new tools here, just looking to see whether the existing C-based APR tools provide good answers within the survey response provided constraints.	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ICSE	Jiang, Nan, Thibaud Lutellier, and Lin Tan	Cure: Code-aware neural machine translation for automatic program repair.	2021	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9401997	7	Jiang, Nan, Thibaud Lutellier, and Lin Tan. "Cure: Code-aware neural machine translation for automatic program repair." 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021.	automated program repair; software reliability;	Computer bugs; Maintenance engineering; Benchmark testing; Search problems; Software; Software reliability; Machine translation;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	CURE - NMT-based APR tool targeting Java. Correctly fixes 57 D4J bugs and 26 QuixBugs (SoTA at the time of publication)	trains on code from before 2006 (when bugs for D4J and QuixBugs start);  pdr note: authors mention using a pre-trained GPT-PL model, then also state that they also pre-train, confusing AF if you don't read linearly in summary: GPT-1 is the architecture, leveraged from hugging face; the authors train the model for PL work, then fine-tune for APR	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J, QuixBugs	CURE	https://github.com/lin-tan/CURE	https://zenodo.org/record/7030145#.YwvXfFvMI5l	https://github.com/lin-tan/CURE ;  training dataset: https://github.com/lin-tan/CoCoNut-Artifact	https://github.com/lin-tan/CURE	six days for data extraction, ten days for PL model, 12 days for fine tuning APR	2.5 minutes on average to generate 10k patches using 4 GPUs. Validation is parallel across these; they say 16.5 minutes on average per bug	We train and evaluate our models on one 56-core server with one NVIDIA TITAN V and three Xp GPUs			Following [19: CoCoNuT], we remove two Defects4J bugs, Closure 63 and Closure 93, from our evaluation as they are duplicates of other Defects4J bugs. 	We use two widely-used benchmarks, Defects4J (v1.4.0) [38] and QuixBugs [47] for evaluation. Following [19: CoCoNuT], we remove two Defects4J bugs, Closure 63 and Closure 93, from our evaluation as they are duplicates of other Defects4J bugs. 	We use CoCoNuT’s training data shared on GitHub [19] as our patch training data, which is extracted from 45,180 Java projects. These Java projects are a superset of the projects used for PL training data since we need more projects to extract enough patch data and it is too expensive to use all these projects for PL training. Then we discard the instances whose context or correct fixes are longer than 1,024 tokens after subword tokenization. Removing instances from the training set is a common practice for machine learning, and since the test set (bugs in Defects4j and QuixBugs are untouched), this setup is valid. Our patch training data contains 2.72 million training instances and 16,920 validation instances.	4.04 million methods from 1,700 open-source projects–to capture code syntax and developer-like source code;  We download all (1,700) open-source Java projects from GitHub that have at least one commit before the first bug in Defects4J according to GHTorrent [42] and roll them back to a version before 2006 to avoid using future data. Then, we use JavaParser [43] to extract all methods except abstract methods and those longer than 1,024 tokens. The PL training data contains 4.04 million methods, with 30,000 instances for validation.	FALSE	TRUE	TRUE	NMT / GPT-1 / GPT-PL	TRUE	TRUE	they cite attention is all you need and specify they use 6 attention heads. Model code is available for more detail	50k vocabulary size. we use an embedding size of 384, eight layers of transformer blocks, and six attention heads. We train GPT for five epochs, using a batch size of 12. We use Adam optimizer [44], and the learning rate increases from 0 to 2.5e−4 at the first 2,000 training steps and then decreases using a cosine schedule.	entire buggy method as input, with buggy line annotated	FALSE			TRUE	assumes perfect line localization, takes method as context	no history or auxiliary data needed (beyond what is used for training); takes buggy method and tries to generate a patch that is the same size (roughly) as the annotated buggy line	summary results only in paper; detailed results in an excel file on the repo	TRUE	TRUE	semantic equivalence; two authors must agree (92% agreement), discuss to resolve other 8%.		vocabulary size = 50,000 beam size = 1,000
ICSE	Li, Yi, Shaohua Wang, and Tien N	Dlfix: Context-based code transformation learning for automated program repair.	2020	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9284100	8	Li, Yi, Shaohua Wang, and Tien N. Nguyen. "Dlfix: Context-based code transformation learning for automated program repair." Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 2020.	Deep Learning; Automated Program Repair; Context-based Code Transformation Learning;	Deep learning; Computer bugs; Tools; Maintenance engineering; Benchmark testing; Context modeling; Software engineering;	FALSE	TRUE	FALSE	TRUE	TRUE	Joe	DLFix uses a two-tier deep learning model (tree-based RNN for context, second layer for translation) that fixes bugs in Java (D4J and Bugs.jar). Fix patterns are learned, instead of hardcoded a la TBar.	Two layer: first is a tree-based RNN for encoding context. That feeds to second layer, which is a slightly tweaked NMT application. They feed it code transformations from prior fixes, and it selects from those, instead of trying to learn them directly.	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J, Bugs.jar	DLFix	https://github.com/ICSE-2019-AUTOFIX/ICSE-2019-AUTOFIX	not available	Improving Bug Detection via Context-Based Code Representation Learning and Attention-Based Neural Networks (SPLASH 2019 - OOPSLA) and https://github.com/OOPSLA-2019-BugDetection/OOPSLA-2019-BugDetection	https://github.com/ICSE-2019-AUTOFIX/ICSE-2019-AUTOFIX	 hours to generate and validate patches (matching what SimFix did)	4-core Intel CPU and a single GTX Titan GPU	5 hours per bug							FALSE	FALSE	TRUE	Two layer: first is a tree-based RNN for encoding context. That feeds to second layer, which is a slightly tweaked NMT application. They feed it code transformations from prior fixes, and it selects from those, instead of trying to learn them directly.	TRUE	TRUE	They essentially build a custom one, with their code context input.	unspecified	100-200 tokens (depending on experiment)	FALSE		Defects4J, Bugs.jar	FALSE	Uses SBFL w/ Ochiai	Takes input from new dataset called BugFix; 4.9M Java methods, 1.8M of which are buggy. Limited to one-line bugs inside of methods, which gets 20K bug/fix pairs that are single-hunk.	per-bug in paper and code repo	TRUE	TRUE	semantic equivalence to human repair	5hr termination	 We tuned DLFIX with the following key hyperparameters using beam-search, such as the vector length of word2vec (100, 150, 200), learning rate (0.001, 0.005, 0.01), and Epoch size (100, 200, 300).
ICSE	Liu, Kui, et al	On the efficiency of test suite based program repair: A systematic assessment of 16 automated repair systems for java programs.	2020	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9283931	9	Liu, Kui, et al. "On the efficiency of test suite based program repair: A systematic assessment of 16 automated repair systems for java programs." Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 2020.	Patch generation; Program repair; Efficiency; Empirical assessment;	Java; Systematics; Computer bugs; Maintenance engineering; Tools; Search problems; Software engineering;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	This is a large scale study that measures the efficiency of APR tools for Java. They focus on the strategy for traversing the search space, for minimizing test effort for validation, and for prioritization of correct patch generation.	They find that wrong location is often cause of irrelevant patches; template based systems perform well for repair but are not efficient.	FALSE	FALSE	FALSE	TRUE	TRUE	FALSE	TRUE	Defects4J		https://github.com/SerVal-DTF/APR-Efficiency.git			https://github.com/SerVal-DTF/APR-Efficiency.git	10k patches per tool (16 tools)	time was not limited	10k patches generated/evaluated at most per bug per tool		jGenProg, GenProg-A, jMutRepair, kPAR, RSRepair-A, jKali, Kali-A, Dynamoth, Nopol, ACS, Cardumen, ARJA, SimFix, FixMiner, AVATAR, TBar					FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J, Bugs.jar	FALSE	Presents results for both normal and perfect FL scenarios		giant repo of per-bug patches generated by 16 tools	TRUE	TRUE	carefully discussed: they have a set of 15 criteria for correct patches if not identical to human (basically codifying what "semantically equivalent" means)		
ICSE	Hong, Seongjoon, et al	SAVER: scalable, precise, and safe memory-error repair.	2020	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9284096	10	Hong, Seongjoon, et al. "SAVER: scalable, precise, and safe memory-error repair." Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 2020.	Program repair; Program analysis; Memory errors; Debugging;	Memory management; Static analysis; Maintenance engineering; Tools; Flow graphs; Open Source software; Software engineering;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	SAVER targets memory errors in C programs (e.g., memory leak, double-free, use-after-free). Method uses an "object flow graph" which "summarizes the program's heap-related behavior using static analysis."	Built using the front-end of INFER (an open source Facebook tool) to translate C programs to SIL IR. Tool written in OCaml. Custom benchmark of 10 open-source C programs. NB: program released only as a binary blob.	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	custom, unnamed, part is inherited from another tool's evaluation (FootPatch), part is new from GitHub	SAVER	GitHub - kupl/SAVER_public NB: binary blob and execution instructions			GitHub - kupl/SAVER_public NB: binary blob and execution instructions	runtime total across all programs is on the order of 1.5 hours. Comparator tool FootPatch takes about two hours.	All experiments were done on a virtual machine running Ubuntu-16.04 with 4 virtual CPUs and 32GB memory, powered by Intel Corei7-7700 processor.	worst individual project takes 1 hour to run							FALSE	FALSE	FALSE		FALSE	FALSE				TRUE	C programs with memory errors; part inherited from another paper, part new (scraped from GitHub and filtered for criteria)	custom, not named / published as a benchmark on its own	FALSE	uses static analysis to identify memory errors; not test-case driven			FALSE	TRUE	"correct" seems to mean "no longer has detected memory errors"		
ICSE	Le, Xuan-Bach D	On reliability of patch correctness assessment.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8812054	11	Le, Xuan-Bach D., et al. "On reliability of patch correctness assessment." 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019.	Automated program repair; empirical study;  test case generation;	Maintenance engineering; Tools;Gold; Reliability; Task analysis; Software; Best practices; 	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	This paper covers an important topic in APR: correctness assessment. Here, they focus on two kinds: auotmated (e.g., held out tests) where correctness is essentially defined as generalizes to new tests; and author annotation.	User study: 35 professional devs were used to annotate patch correctness. They used inter-rater agreement as a proxy for annotation quality.	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	FALSE	Patches generated by ACS, Kali, GenProg, Nopol, S3, Angelix, and Enumerative and CVC4 embedded in JFix. D4J & S3's dataset															FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J, S3 paper's bugs	FALSE				TRUE	TRUE	This paper is all about correctness: either developer-labeled, or test-suite-labeled.		
ICSE	Maoz, Shahar, Jan Oliver Ringert, and Rafi Shalom	Symbolic repairs for GR (1) specifications.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8812056	12	Maoz, Shahar, Jan Oliver Ringert, and Rafi Shalom. "Symbolic repairs for GR (1) specifications." 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019.	reactive synthesis; repair; GR(1);	Maintenance engineering; Safety; Glass; Cascading style sheets; Benchmark testing; Scalability; Standards;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	This is a paper on symbolic model synthesis for robotics and aims to "repair" unrealizable constraint sets. I wouldn't classify this as APR in the way we're discussing it, though it fits a broader definition.	"We are partly inspired by this body of work [Joe note: APR]. However, we do not deal with repairing imperative programs but rather with repairing GR(1) specifications, which are temporal declarative specifications for reactive systems."	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ICSE	Ghorbani, Negar, Joshua Garcia, and Sam Malek	Detection and repair of architectural inconsistencies in java.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8812079	13	Ghorbani, Negar, Joshua Garcia, and Sam Malek. "Detection and repair of architectural inconsistencies in java." 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019.	Java Platform Module System; Architectural Inconsistencies; Static Program Analysis; Software Architecture; Module; Detection; Repair; Security; Maintainability; Encapsulation; Software Bloat; Java;	Java; Computer architecture; Software; Bars; Encapsulation; Security; Maintenance engineering;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	This introduces DARCY, a static analysis informed approach that detects and repairs inconsistent dependencies (largely differences between module declarations and usage of modules in an application). Evaluated on 38 Java-9 applications.	Not really bugs as much as efficiency and potential security considerations.	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ICSE	Saha, Seemanta	Harnessing evolution for multi-hunk program repair.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8812131	14	Saha, Seemanta. "Harnessing evolution for multi-hunk program repair." 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019.	automatic program repair; multi-hunk patches; code similarity; 	Maintenance engineering; Computer bugs; Cloning; Tools; History; Search problems; Syntactics;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	This is the Hercules paper. Targets multi-hunk bugs by analyzing code to identify siblings, then abstracting a template-based fix and concretizing it using in-scope ingredients at each location.	Needs project history, does some nice analysis. Their motivating example is a bad one: they synthesized a function call to code that fixes the bug (not fixing it directly). The issue is that this code was from the fixed version of the program, and the D4J maintainers left it in the artificially-constructed buggy version.	TRUE	FALSE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J	Hercules	closed source	no model provided	trained on Bugs.jar (sans the projects also in D4J)	not available			5 hours on a 3.6ghz double core processor with 4GB ram; Ubuntu 16.04, Java 7. Timeout 5 hours (top 200 locations from SBFL got 50 candidate patches at most per repair schema)							FALSE	FALSE	TRUE	unspecified (machine learning based patch ranking model)	FALSE	FALSE	no	unspecified	unspecified	FALSE		Defects4J	FALSE	uses SBFL + some enhancements to search for code siblings	program history	neither; code and data not available. got a spreadsheet from the authors on request	TRUE	TRUE	semantic equivalence		n/a
ICSE	Tomassi, David A	Bugswarm: Mining and continuously growing a dataset of reproducible failures and fixes.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8812141	15	Tomassi, David A., et al. "Bugswarm: Mining and continuously growing a dataset of reproducible failures and fixes." 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019.	Bug Database; Reproducibility; Software Testing; Program Analysis; Experiment Infrastructure;	Tools; Software maintenance; Libraries; Open source software; Software quality; Java; Python; 	FALSE	TRUE	FALSE	TRUE	TRUE	Joe	BugSwarm is a tool for mining defects and their repairs using Travis CI	It's an interesting way to get current and diverse bugs. Not widely used though, I don't think.	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ICSE	Xu, Xuezheng, et al	VFix: value-flow-guided precise program repair for null pointer dereferences.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8812101	16	Xu, Xuezheng, et al. "VFix: value-flow-guided precise program repair for null pointer dereferences." 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019.	program repair; static analysis; null dereference;	Maintenance engineering; Computer bugs; Tools; Aerospace electronics; Software; Australia;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	VFix aims to fix a narrow set of bugs well. They consider only null pointer exceptions and evaluate on D4J where they outperform other tools on this class of bugs.	I feel like we need a bunch of papers like this. They've thought carefully about a specific class of bugs, and repair it efficiently and well. Very unlike more general approaches where, if it works, who knows why. Feels like the kind of thing that builds knowledge in the SE community.	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J, plus some other Apache projects	VFix	code used to be available, but is not anymore?				10 minutes per bug (extremely efficient)		10 minutes per bug; if it finds a repair, it's almost always the first thing they try							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J, also some other Apache projects	FALSE	value-flow analysis is used to identify potential null pointer dereferece exceptions		per-bug	TRUE	TRUE	A patch is correct iff it passes all the test cases in the test suite and is also semantically or syntactically equivalent to a human-written patch		
ICSE	Wen, Ming, et al	Context-aware patch generation for better automated program repair.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453055	17	Wen, Ming, et al. "Context-aware patch generation for better automated program repair." Proceedings of the 40th international conference on software engineering. 2018.	Context-Aware; Automated Program Repair; Patch Prioritization;	Computer bugs; Maintenance engineering; Search problems; Explosions; Context modeling; Software; Benchmark testing;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	CapGen - context-aware patch generation for Java. Prioritizes mutation operators (works at AST level), ranks patches before evaluating.	Evaluated on Defects4J and IntroClassJava. Statistical model (not ML) used to select repair operations (data crunched from BugFixSet). Ingredients selected according to probability.	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J and IntroClassJava	CapGen	https://github.com/MingWEN-CS/CapGen			https://github.com/MingWEN-CS/CapGen	90 minutes per bug	Our experiments are run on a CentOS server with 2x Intel Xeon E5-2450 Core CPU @2.1GHz and 192GB physical memory	90 minutes							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J, IntroClassJava	FALSE	More sophisticated than just SBFL: picks fault space, operator space, and fix ingredient space separately; ranks patches by the multiplied outputs of a statistical model for each of those		per-bug	TRUE	TRUE	semantic equivalence		
ICSE	Hua, Jinru, et al	Towards practical program repair with on-demand candidate generation.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453056	18	Hua, Jinru, et al. "Towards practical program repair with on-demand candidate generation." Proceedings of the 40th international conference on software engineering. 2018.	debugging; program repair; program synthesis; lazy initialization; execution driven pruning; 	Maintenance engineering; Computer bugs; Runtime; Syntactics; Debugging; Java;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	SketchFix - instead of generating a bunch of patches, then evaluating all at once (common for other methods), they lazily generate patches to reduce compile time and use runtime info of tests		TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J	SketchFix	https://github.com/SketchFix/SketchFix				not explicitly stated that I saw; worst individual bug was 208.5 minutes (so at least 3.5 hours)	All performance experiments are conducted on a platform with 4-core Intel Core i7-6700 CPU (3.40 GHz) and 16 Gigabyte RAM on Ubuntu 16.04	not stated, but at least 3.5 hours based on worst report in table							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J	FALSE	SBFL using Ochiai (limited to top 50)		per-bug	TRUE	TRUE	semantic equivalence		
ICSE	Yi, Jooyong, et al	A correlation study between automated program repair and test-suite metrics.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453057	19	Yi, Jooyong, et al. "A correlation study between automated program repair and test-suite metrics." Proceedings of the 40th International Conference on Software Engineering. 2018.	Code Coverage; Program Repair; Mutation Testing;	Maintenance engineering; Measurement; Correlation; Software reliability; Software; Software testing;	TRUE	FALSE	FALSE	FALSE	TRUE	Joe	This paper asks whether test suite metrics from software testing can be used for APR, e.g., whether they meaningfully correlate to repair correctness (the only kind of quality they explicitly look at).	This is a presentation of a journal-first paper that appeared in 2017 in EMSE. Document contains basically no details. Original paper: J. Yi, S. H. Tan, S. Mechtaev, M. Böhme, and A. Roychoudhury. 2017. A Correlation Study between Automated Program Repair and Test-Suite Metrics. EMSE (2017).	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ICSE	Motwani, Manish, et al	Do automated program repair techniques repair hard and important bugs?.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453058	20	Motwani, Manish, et al. "Do automated program repair techniques repair hard and important bugs?." Proceedings of the 40th International Conference on Software Engineering. 2018.	Automated program repair; Repairability;	Maintenance engineering; Java; Computer bugs; Correlation; Software engineering; Complexity theory; Benchmark testing;	TRUE	TRUE	FALSE	FALSE	TRUE	Joe	This is a journal-first paper as well. It is a summary of the presentation given at ICSE of the data contained in a 2018 EMSE paper (which is row 99 on our list): M. Motwani, S. Sankaranarayanan, R. Just, and Y. Brun. Do automated program repair techniques repair hard and important bugs? EMSE, 2018 		FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ICSE	Bhatia, Sahil, Pushmeet Kohli, and Rishabh Singh	Neuro-symbolic program corrector for introductory programming assignments.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453063	21	Bhatia, Sahil, Pushmeet Kohli, and Rishabh Singh. "Neuro-symbolic program corrector for introductory programming assignments." Proceedings of the 40th International Conference on Software Engineering. 2018.	Neural Program Correction; Automated Feedback Generation; Neural guided search;	Syntactics; Maintenance engineering; Programming; Recurrent neural networks; Prediction algorithms; Semantics;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	This paper uses RNNs to perform syntax repairs for buggy student programs submitted to MOOCs. It repairs syntax in 60% of cases, and finds correct repairs for 23.8%. 14.5k student submissions in their dataset.	Tool is titled SynFix	TRUE	TRUE	TRUE	FALSE	FALSE	FALSE	FALSE	custom benchmark of 14.5k student submissions from MOOCs	SynFix	could not find				unspecified	unspecified	no time limit or resource usage provided							FALSE	FALSE	TRUE	RNN, tokens encoded as one-hot, w/ fixed vocab size. Rare words substituted with IDENT token.	TRUE	FALSE		"We use RNN with LSMT cells consisting of 2 hidden layers each with 128 hidden units and train the model for 50 epochs."	sequence length is 10, vocab size not specified; varies in tabular results between 117 and 1053 for different problems.	FALSE		custom, not released	FALSE	guided by syntax error location given by python interpreter		very summary reporting	TRUE	TRUE	I believe "plausible" ~ syntactically correct, "correct" ~ passes test cases.		n/a
ICSE	Mechtaev, Sergey, et al	Semantic program repair using a reference implementation.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453071	22	Mechtaev, Sergey, et al. "Semantic program repair using a reference implementation." Proceedings of the 40th International Conference on Software Engineering. 2018.	Debugging;Program repair; Verification;	Maintenance engineering; Software; Semantics; Forestry; Instruments; Signal processing algorithms; Scalability;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Attempts to compensate for lack of formal specification by inferring semantics from a reference piece of software. They use the example of Linux Busybox v. GNU Coreuitls. Probably limited to ensuring semantic consistency between implementations, but one has to be assumed correct. doesn't use test suites.		TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	Custom collection of program pairs, each of which is meant to satisfy the same spec.	SemGraft	could not find				~45 minutes per program		reported only as "45 minutes" on average							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		custom set of progam pairs where the programs satisfy the exact same spec	FALSE	good question. they provide a specially crafted precondition (input) to the program, then look for differences in the semantic use of those variables. I think.	needs a reference implementation (i.e., a correct program must already exist....)		FALSE	TRUE	syntactic equivalence (modulo refactoring)		
ICSE	Mahajan, Sonai, et al	Automated repair of mobile friendly problems in web pages.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453072	23	Mahajan, Sonai, et al. "Automated repair of mobile friendly problems in web pages." Proceedings of the 40th International Conference on Software Engineering. 2018.	Mobile Friendly Problems; automated repair; web apps;	Web pages; Maintenance engineering; Mobile handsets; Cascading style sheets; Layout; Navigation; Organizations;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Custom tool (MFix) to repair mobile view issues with websites. Evaluation is on 38 of the Alexa Top 50 most visited websites. These are then scored automatically by "mobile friendliness rating." Fixes issues in CSS, where "buggy" behavior is "looks bad on mobile" essentially.	5 minutes on average per CSS patch. User study showed that the repaired version was "overwhelmingly preferred". Fault localization by Google Mobile-Friendly Test Tool, an oracle that reports whether there are any types of mobile friendly problems present in the page. Returns reference to HTML elements that contain that problem. 	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	Alexa Top 50 sites	MFix	could not find				5 minutes per repair	We parallelized the evaluation of candidate solutions using a alcoud of 100 Amazon EC2 t2.xlarge instances pre-installed with Ubuntu 16.04	5 minutes per patch (but parallelized over 100 beefy computers)							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Alexa Top 50 website code	FALSE	uses an oracle to identify whether mobile view issues exist on a page; localizes further via segmentation (clustering)			FALSE	TRUE	Not clear how this applies; absence of the oracle indicating faults might be "correct" but would "plausible" be "looks better but maybe still has errors"?		
ICSE	van Tonder, Rijnard, and Claire Le Goues	Static automated program repair for heap properties.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453073	24	van Tonder, Rijnard, and Claire Le Goues. "Static automated program repair for heap properties." Proceedings of the 40th International Conference on Software Engineering. 2018.	Automated Program Repair; Separation Logic;	Computer bugs; Maintenance engineering; Tools; Static analysis; Semantics; Software; Safety;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Program repair without test cases: uses static analysis and Separation Logic to reason about faults related to pointer safety: resource leaks, memory leaks, and null dereferences. Fixes 55 bugs, including 11 previously undiscovered bugs, in 11 real-world projects.	This is an important paper: APR w/o test suites. Test case availability and quality is a big problem in APR. We need more like this	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	custom dataset of 11 real-world projects	FootPatch	https://github.com/squaresLab/footpatch				worst in table is 20 minutes	Ubuntu 16.04 LTS server with 20 Xeon E5-2699 CPUs and 20 GB RAM	20 mins was worst in table							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		custom; 11 large-scale projects in Java and in C	FALSE	uses Infer (facebook open source tool) to look for static analysis violations		per-bug	FALSE	TRUE	some were to new bugs that their method discovered and fixed; correctness was verified by manual analysis from text:  "in practice, we apply each patch generated by FootPatch and rerun the static analyzer to see if the patch removes the bug (all did). Where possible, we ran a project’s test suite after applying our patches to validate that our changes do not break tests. We successfully ran the test suites for Apk-tool, armake, and error-prone, which pass. Two projects contained no tests, and the remaining six test suites could not be successfully configured/built."		
ICSE	Le, Xuan-Bach D	Overfitting in semantics-based automated program repair.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453074	25	Le, Xuan-Bach D., et al. "Overfitting in semantics-based automated program repair." Proceedings of the 40th International Conference on Software Engineering. 2018.	Automated Program Repair; Program Synthesis; Symbolic Execution; Patch Overfitting;	Maintenance engineering; Semantics; Benchmark testing; Software engineering; Tools; Engines; Sociology;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	This paper uses IntroClass and CodeFlaws to characterize overfitting in patches generated by semantics-based APR tools.	Journal-first paper: Empir Software Eng (2018) 23:3007–3033 https://doi.org/10.1007/s10664-017-9577-2	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ICSE	Xiong, Yingfei, et al	Identifying patch correctness in test-based program repair.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453152	26	Xiong, Yingfei, et al. "Identifying patch correctness in test-based program repair." Proceedings of the 40th international conference on software engineering. 2018.	Test based program repair; patch correctness; patch classification; test generation;	Maintenance engineering; Search problems; Software; Computer crashes; Null value; Runtime;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Test suites are flawed proxies that don't guarantee correctness. This paper attempts to automatically augment the test suite to reduce the fraction of plausible patches that are not also correct. It uses 139 patches to Java programs from some early Java APR tools		FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	FALSE	139 patches generated by jGenProg, Nopol, jKali, HDRepair, and ACS	Patch-Sim / Test-Sim	https://github.com/Ultimanecat/DefectRepairing				3 minutes per bug for test generation	Intel Xeon E3 CPU and 32GB memory	3 minutes per patch for test suite augmentation, plus normal runtime of tests on patch (ballpark 5 minutes per Defects4J bug from Joe's experience)							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		set of patches produced by extant tools (taken from their published data)	FALSE				TRUE	TRUE	Patches that are correct are correct the same way; patches that are plausible are often plausible in different ways. Distance metrics used for automatic filtering based on this.		
ICSE	Hassan, Foyzul, and Xiaoyin Wang	Hirebuild: An automatic approach to history-driven repair of build scripts.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453189	27	Hassan, Foyzul, and Xiaoyin Wang. "Hirebuild: An automatic approach to history-driven repair of build scripts." Proceedings of the 40th international conference on software engineering. 2018.	Patch Generation; Software Build Scripts; Build Logs;	Maintenance engineering; Software; Tools; Task analysis; Computer bugs; Data mining; Software engineering;	FALSE	FALSE	FALSE	TRUE	TRUE	Joe	HireBuild - repair of Gradle build scripts for Java projects. They note that based on Travis CI data, 22% of code commits include changes to build script files. Targets repair (defined as compilation) of these.	Dataset is from TravisTorrent dataset, filtered for Gradle build script changes that fix build failures. Patch ranking: "we give higher priority to the patches which involve values or scopes more similar to the buggy script and the build-failure log"	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	custom set of Gradle build failures (pair of failure and patch taken from TravisTorrent)	HireBuild	https://sites.google.com/site/buildfix2017/		https://sites.google.com/site/buildfix2017/	https://sites.google.com/site/buildfix2017/	600 mins as timeout	2.4GHz Intel Core i7 CPU w/ 16GB ram, Ubuntu 14.10	5h time limit							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		scraped data from TravisTorrent, filtered for Gradle build fail/fix pairs	FALSE	chooses first .gradle file mentioned in error log; if none, uses build.gradle in root folder	mines a bunch of fix patterns from the 135 chronologically oldest bug/fix pairs in their dataset; targets the 40 remaining (of which 24 were reproducible bugs)	per-bug	FALSE	TRUE	Correctness = project builds now where it didn't before. Behavior at runtime is not tested.		
ICSE	Gazzola, Luca, Daniela Micucci, and Leonardo Mariani	Automatic software repair: A survey.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453203	28	Gazzola, Luca, Daniela Micucci, and Leonardo Mariani. "Automatic software repair: A survey." Proceedings of the 40th International Conference on Software Engineering. 2018.	Automatic Program Repair; Generate and Validate; Search Based; Semantics driven repair; Correct by Construction; Program Synthesis; Self Repairing; 	Maintenance engineering; Debugging; Software engineering; Fault diagnosis; Automation; Software systems; 	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	This is the Gazzola APR Survey paper (#54 on this list) announcement at ICSE. 	L. Gazzola, D. Micucci, and L. Mariani. to appear. Automatic software repair: A Survey. IEEE TSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Chen, Zimin, Steve Kommrusch, and Martin Monperrus	Neural transfer learning for repairing security vulnerabilities in c code.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9699412	29	Chen, Zimin, Steve Kommrusch, and Martin Monperrus. "Neural transfer learning for repairing security vulnerabilities in c code." IEEE Transactions on Software Engineering 49.1 (2022): 147-165.	vulnerability fixing; transfer learning; seq2seq learning;	Transfer learning; Task analysis; Computer bugs; Transformers; Codes; Training; Software;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	VRepair - transfer learning for security vulerabilities. Targets C functions.	They scraped all of GitHub between for all of 2017 and 2018 (892 million events, of which 478 million were pushes). Found 21 million bug fixing commits in C code. Trained on that. This paper is insane.	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	FALSE	Big-Vul, CVEfixes	VRepair	https://github.com/SteveKommrusch/VRepair	https://github.com/ASSERT-KTH/VRepair	https://github.com/ASSERT-KTH/VRepair	https://github.com/ASSERT-KTH/VRepair			completely unreported as far as I could tell							FALSE	FALSE	TRUE	NMT / encoder-decoder transformer  (attention is all you need) 	TRUE	TRUE	"Multiple copies of multi-head attention layers learn hidden representations of the input data. These representations are then used by a second set of multi-head attention layers to produce a table of probabilities for the msot likely token to output."	Trained models are a couple hundred gigs, so the models themselves were beefy.	vocab size ranged from 2k to 10k	FALSE		massive set of 21 million bug fixing commits to C code scraped from GitHub used for training; vuln datasets listed earlier used for transfer learning	TRUE	FL data reported mostly for First vuln line (though ablation study also provides top level summary of no vuln line ID, single block ID, or all vulnerable lines ID'd)		per-defect-class	FALSE	TRUE	As far as I can tell, the metric here is "accuracy" which means "did the neural net spit out the same string as was in the original dataset"?		beam size = 50;  vocabulary size = 5000
TSE	Winter, Emily, et al	Let’s talk with developers, not about developers: A review of automatic program repair research.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9714799	30	Winter, Emily, et al. "Let’s talk with developers, not about developers: A review of automatic program repair research." IEEE Transactions on Software Engineering 49.1 (2022): 419-436.	Human factors; software development; automated program repair;	Human factors; Software; Software engineering; Maintenance engineering; Bibliographics; Systematics; Technological innovation;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Literature review of human study usage in APR and its shortcomings. Has recommendations on how to do it better.	Not an eval, so skipping subsequent	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Winter, Emily, et al	How do developers really feel about bug fixing? Directions for automatic program repair.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9842369	31	Winter, Emily, et al. "How do developers really feel about bug fixing? Directions for automatic program repair." IEEE Transactions on Software Engineering (2022).	activity APR; APR tools; automatic program repair; bug finding; developer community; developer experience; fixing practices; manual bug fixing; scale APR adoption;	Computer bugs; Software; Debugging; Automation; Maintenance engineering; Task analysis; Manuals;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	Large survey of developers (386 of them). Shows that bug fixing is actually rated as more enjoyable than other parts of their work, so automating might not drive adoption. Also shows strong preference for developer to be kept in the loop (e.g., choosing between fixes, or validating fixes)	Not an eval, so skipping subsequent	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Chen, Liushan, et al	Program repair with repeated learning.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9749899	32	Chen, Liushan, et al. "Program repair with repeated learning." IEEE Transactions on Software Engineering 49.2 (2022): 831-848.	Automated program repair; generate-and-validate; learning-to-rank; repeated learning;	Java; Fitting; Computer bugs; Machine learning; Maintenance engineering; Benchmark testing; Context modeling;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Liana is a variation on generate and validate using a model - it loops, using runtime information to prejudice what is generated next	Builds upon Restore (w/ generalization shown by building on SimFix instead)	TRUE	TRUE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J, IntroClassJava, and QuizBugs	Liana														FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Chi, Jianlei, et al	Seqtrans: Automatic vulnerability fix via sequence to sequence learning.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9729554	33	Chi, Jianlei, et al. "Seqtrans: Automatic vulnerability fix via sequence to sequence learning." IEEE Transactions on Software Engineering (2022).	Machine learning; neural machine translation; software engineering; vulnerability fix;	Maintenance engineering; Codes; Computer bugs; Predictive models; Transformers; Decoding; Training;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	This paper introduces SeqTrans, which is an NMT model for program repair. It focuses on fixing security bugs in Java projects, using the Transformer model and a fine-tuning strategy.	Security focused APR using very standard NMT. The special sauce is in using data-flow dependencies to define context and fine tuning the trained model. Better at fixing some defect types than others.	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	FALSE	Learned from Tufano's large set of bug-fixing commits; applied and tested vs SAP's large collection of CVEs	SeqTrans	https://github.com/chijianlei/SeqTrans	https://drive.google.com/drive/folders/1gzUVyKoEFtK55hZcBlNL004oB9El6RzI	Data - Learning-Fixes          SAP data	https://drive.google.com/drive/folders/1gzUVyKoEFtK55hZcBlNL004oB9El6RzI	300k pretraining steps, 100k fine tuning steps. Real-world time not specified	All experiments were accomplished on a server with an Intel Xeon E5 processor, four Nvidia 3090 GPU, and 1 TB RAM	beam size 50 at most							FALSE	FALSE	TRUE	OpenNMT / transformer / encoder-decoder	TRUE	TRUE	Describes model in detail; attention is a function of the query vector, the key vector, and a value vector. All are trained. (This is off the shelf I think but they describe it kinda)	if it's specified, I missed it	0 to 1500 was the range of context token sizes for the datasets they use	FALSE			TRUE	Perfect	They sub out identifier names for generics (var1, var2; num1, num2; "liter" for strings). Vocab size is 8k	summary in paper, details in data dump	TRUE	TRUE	"manual inspection" after passing tests		vocabulary size = 8k beam size = [1 to 50]
TSE	Ahmed, Toufique, Noah Rose Ledesma, and Premkumar Devanbu	SYNSHINE: improved fixing of Syntax Errors.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9913705	34	Ahmed, Toufique, Noah Rose Ledesma, and Premkumar Devanbu. "SYNSHINE: improved fixing of Syntax Errors." IEEE Transactions on Software Engineering (2022).	Deep learning; program repair; naturalness;	Syntactics; Maintenance engineering; Program processors; Codes; Java; Transformers; Data models;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	SynShine tries to provide automated repair suggestions for syntax errors to aid novices. Uses compiler diagnostics as an auxiliary input to the program text. Implemented into a free version of VSCode; source and data public.	Syntax errors cause all tests to fail, becuase the program fails to compile, or halts during interpretation. Fixing them is repairing code, even if it's pre-deployment in this case. Combines 3 learned networks with the Java compiler in a pipeline. BlockFix is a transformer model which handles block-related issues. LineFix, BERT-derived, uses javac output to identify the buggy line and fixes it if it can, or passes a token to UnkFix to handle unknown identifiers. LineFix outputs one of 154 edit commands (insert/delete/substitute delimiters, keywords, operators, or identifiers). UnkFix is a DNN language model to suggest renaming or inserting identifier names.  Authors use RoBERTa as their LLM model, but pre-train the model themselves with a dataset they've collected.	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	FALSE	Trained on 5k most starred GitHub java projects. Fine-tuned on Blackbox repository (1.7m paris of buggy and fixed programs)	SynShine	https://zenodo.org/record/7789329	https://zenodo.org/record/7789329	Blackbox data is on-request, it seems. UK law governing student data or some such.	additional data https://drive.google.com/file/d/1m9Dvb4SfEsRj0dQx-ft4c4C2ipjdjcLr/view		Two NVIDIA Titan RTX GPUs for five epochs with a batch size of 44 sequences and a learning rate 5e-5 (specified for one model only and one part of training)	once trained, very efficient. They report an average response time of 0.88 seconds once the user clicks the button to invoke their routine. Very impressive, IMO				see fine-tuning dataset	For fine-tuning and then for evaluation, we used realistic novice programs with syntax errors and human-produced fixed versions. We used the exact dataset used by Santos et al. [8] and Ahmed et al. [9] from the Blackbox [11] repository. This dataset contains 1.7M pairs, of erroneous and fixed programs. Both Santos et al. and Ahmed et al. primarily report their performance on programs with a single token error because a single edit can fix a large fraction of the programs (around 57%). Therefore, for a fair comparison, we also initially focused our evaluation on single token errors and broke down our performance by token-length, as done by Ahmed et al. We selected a test set of 100K samples, with samples stratified by length, from the full dataset for the evaluation. We divided the test dataset into ten token-length ranges (lengths of 1-100, 101-200, ..., and 900-1000 tokens), with each range having around 10K examples. We prepare our fine-tuning dataset from the remaining examples.	To generate the dataset for pre-training, we collected 5000 most starred Java projects from GitHub (since our end-goal is to correct Java syntax errors). We tokenized the files, yielding 1.2 billion tokens for the pre-training. For the MLM pre-training over code, we randomly select 15% of tokens, and replace with a unique token mask. The loss here is the cross-entropy of the original masked token. Of the 15% selected tokens, 80% are replaced with a specific marker mask, 10% are left unchanged, and a randomly selected token replaces the remaining 10%. This training method follows the standard RoBERTa protocol.	FALSE	TRUE	TRUE	RoBERTa w/ MLM softmax layer for LineFix, MLM (masked language model) for UnkFix, off the shelf BF+FF model for basic block fixing (not trained here, unspecified)	TRUE	TRUE	Nothing speical here, uses RoBERTa for that. Auxiliary input of compiler to decide where to focus (sort of fault localization)	not stated explicitly; the VSCode implementation "code correction server" (model API) uses 1.765 GB memory	150, though Table 2 shows detailed data for 1-1k, in increments of 100	FALSE		Trained on GitHub projects, tested on Blackbox datasets	FALSE	Uses java compiler output, so essentially an oracle for a line	runs java compiler on code at runtime to get its output to use for localization information (and token identification, where error messages are specific). Not additional required input, since it is run by the tool itself.	summary	FALSE	TRUE	correctness not exactly detailed; but here plausible might mean "the compiler no longer complains about the code" and correct adds "did the same thing as the Blackbox example repair"		
TSE	Motwani, Manish, et al	Quality of automated program repair on real-world defects.	2020	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9104918	35	Motwani, Manish, et al. "Quality of automated program repair on real-world defects." IEEE Transactions on Software Engineering 48.2 (2020): 637-661.	Automated program repair; Patch quality; Objective quality measure; Java; GenProg; Par; TrpAutoRepair; Defects4J;	Maintenance engineering; Java; Contracts; Manuals; Diversity reception; Inspection; Software quality;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	This paper is about overfitting; publishes JaRFly (faithful implementation of GenProg for Java written largely by Claire). Discusses Defects4J.		TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J	JaRFly (aka GenProg4Java)	https://github.com/squaresLab/genprog4java/			https://github.com/LASER-UMASS/JavaRepair-replication-package	20 repair attempts, each with timeout of 4 hours except SimFix which has a default of 5 hours. Total time for 357 defects, 3 techniques, was 10 CPU-years.	50 node cluster, each with Xeon E5-2680 v4 CPU with 28 cores @ 2.40 GHz. 128GB RAM, 200GB SSD.	4 hours, 5 for SimFix, 20 runs each (1 for SimFix)							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J	FALSE	SBFL (standard)		summary, with more in replication package	TRUE	TRUE	The paper is about overfitting, so uses test cases as well as autogenerated test cases.		
TSE	Xu, Tongtong, et al	Restore: Retrospective fault localization enhancing automated program repair.	2020	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9068412	36	Xu, Tongtong, et al. "Restore: Retrospective fault localization enhancing automated program repair." IEEE Transactions on Software Engineering 48.1 (2020): 309-326.	retrospective fault localization; mutation-based dynamic analysis; Jaid Java program repair system; automated program repair tools; program repair process; Defects4J standard benchmark; Restore; program location identification;	Maintenance engineering; tools; Java; Computer bugs; Software; Standards; Electronic mail;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	RESTORE is based on Jaid for Java program repair. They use "retrospective fault localizaton" (aka MBFL?) to refine fault localization using online data from patches that fail to validate. Underlying algorithm is same as Jaid. They also implement their new FL technique for SimFix and find it helps there, too.		TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J	RESTORE	https://drive.google.com/drive/folders/1wMCuFnniVtiurAdJT6yc830WV_jWpa1c			https://drive.google.com/drive/folders/1wMCuFnniVtiurAdJT6yc830WV_jWpa1c	2-3 hours on average per repair, range was from 1.5 minutes and 21 hours; median 53		worst was 21 hours, median 53 minutes							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J	FALSE	MBFL at runtime, SBFL to start		per-bug	TRUE	TRUE	semantic equivalence to human patch		
TSE	Wang, Yu, et al	Automatic Detection, Validation, and Repair of Race Conditions in Interrupt-Driven Embedded Software.	2020	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9072666	37	Wang, Yu, et al. "Automatic Detection, Validation, and Repair of Race Conditions in Interrupt-Driven Embedded Software." IEEE Transactions on Software Engineering 48.1 (2020): 346-363.	Embedded software; interrupts; race condition; software testing; repair suggestion;	Task analysis; Maintenance engineering; Hardware; Embeddd systems; Concurrent computing; Testing; Embedded software;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	APR for interrupt-driven programs in embedded systems. SDRacer detects, validates, and repairs race conditions. Static analysis + symbolic execution to generate input data, then virtual platforms to force interrupts to occur at potential racing points.	Evaluated on 9 embedded programs in C; implemented using Clang Tool 3.4. Symbolic execution implemented based on KLEE 1.2 with some mods to support kernel functions. APR tool itself implemented in Python, using the Simics API to access virtual platform with simulated x86 CPUs. Cool paper!	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE		SDRacer	didn't see (even on googling)				Timeout for symbolic execution set to 10 minutes. Overall time ranged from 2 seconds to 23 minutes.	a single 4-core Intel i5-2400 w/ 8GB RAM on Ubuntu 12.04. Guest OS was 10.04.								FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		custom set of 11 implementations of 9 real-time embedded systems, taken from public sources	FALSE	static analyzer points to potential race conditions			FALSE	TRUE	they reported fixed versions to project maintainers, and all of them were confirmed and incorporated		
TSE	Kechagia, Maria, et al	Evaluating automatic program repair capabilities to repair API misuses.	2021	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9381596	38	Kechagia, Maria, et al. "Evaluating automatic program repair capabilities to repair API misuses." IEEE Transactions on Software Engineering 48.7 (2021): 2658-2679.	Automated program repair; Application programming interfaces; APIs; API misuses; bug benchmarks;	Tools; Maintenance engineering; Computer bugs; Benchmark testing; Software; Java; Security;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	APIARTy is a framework that evaluates multiple repair tools. It tries to automatically repair API misuses. Runs 14 Java repair tools on their new benchmark APIRepBench.	Benchmark construction extracts and curates API-speicifi bugs from Bears, Bugs.jar, and MUBench (which itself pulls from BugClassify, Defects4J and QACrashFix).	FALSE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	APIRepBench		https://github.com/SOLAR-group/APIARTy				2 hour timeout for each tool except Avatar, TBar and SimFix, which got 4 hours. Whole experiment took 36 days.	Ubuntu 16.04 docker images deployed on 2-core PCs with 8GB ram, 3.1GHz i processor.	2-4 hours per tool per bug, 36 days total (parallelism not specified)		ARJA, Kali, GenProg, RSRepair, jGenProg, jKali, jMutRepair, Cardumen, Nopol, DYnaMoth, NPEFix, Avatar, TBar, and SimFix					FALSE	FALSE	FALSE		FALSE	FALSE				TRUE	API misuses (curated from other published benchmarks with a common execution framework)		FALSE		excluded tools that required this	per-bug	TRUE	TRUE	semantic equivalence, with special notes for patches that were wrong but in the right place.		
TSE	Jin, Hai, et al	Aroc: An Automatic Repair Framework for On-chain Smart Contracts.	2021	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9591399	39	Jin, Hai, et al. "Aroc: An Automatic Repair Framework for On-chain Smart Contracts." IEEE Transactions on Software Engineering 48.11 (2021): 4611-4629.	Smart contract; vulnerability; repair; on-chain protection;	Contracts; Smart contracts; Codes; Blockchains; Computer bugs; Maintenance engineering; Task analysis;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Aroc is a repair tool for smart contract repair, which is complicated by the fact that once a contract is published it is immutable.	This is an interesting domain, with unique constraints. It works essentially by generating an additional contract that blocks malicious transactions in advance. Requires the owner of the vulnerable contract to send a transaction through their system to wrap the contract with the patch.	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	dataset of 98 bugs from a prior work, with 500 random contracts selected from EVMPatch dataset, plus two other small sets of known bugs	Aroc	could not find (probably for profit)					2.4 GHz Intel Xeon E5-2630 v3 processor w/ 8 cores, 64 GB RAM, Ubuntu 18.04	unspecified							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		custom set from four sources (detailed earlier)	FALSE	uses analysis methods to discover code vulnerable to known patterns		summary	TRUE	TRUE	correctness is a bit of an artificial notion, here, since they only include contracts with known patterns they can identify; an "imprecise" patch is one that is correct, but doesn't cover all paths. For on-chain protection, they look at false pos, false neg, and "failed" which couldn't be corrected at all		
TSE	Parasaram, Nikhil, Earl T	Trident: Controlling Side Effects in Automated Program Repair.	2021	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9611365	40	Parasaram, Nikhil, Earl T. Barr, and Sergey Mechtaev. "Trident: Controlling Side Effects in Automated Program Repair." IEEE Transactions on Software Engineering 48.12 (2021): 4717-4732.	Program repair; Program synthesis; Symbolic execution; Side effects;	Maintenance engineering; Computer bugs; Codes; Databases; Training data; Switches; Semantics;	FALSE	TRUE	FALSE	TRUE	TRUE	Joe	Trident is a repair tool for C that synthesizes patches that have side effects on purpose, on the principle that side effects are required to fix some bugs. It is evaluated on ManyBugs and fixes two new things.	Evaluated on GNU projects, also ManyBugs, and some Codeflaws. All selected to be bugs that require side effects. (Joe wonders how it would work on actual random samples of bugs). This is not an ML paper, it is a synthesis paper. Fewer plausible patches than GenProg on this subset of bugs (GP: 20, Trident: 16) but more correct (GP 3, Trident 8)	TRUE	TRUE	FALSE	TRUE	TRUE	FALSE	FALSE	Custom combo: 10 bugs from GNU programs, 36 from ManyBugs, and 110 from Codeflaws. Filtered so that the fixes require side effects.	Trident	https://github.com/norhh/Trident-TSE				2 hour timeout	Intel i7-2600 2.7 GHz on Ubuntu 16.04 with 16GB RAM			Prophet, SOSRepair, Angelix, GenProg					FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		10 GNU program bugs, 36 ManyBugs, 110 from Codeflaws (all subsetted so that the repairs require side effects)	FALSE	SBFL w/ Ochiai			TRUE	TRUE	semantic equivalence to human repair		
TSE	Ye, He, et al	Automated classification of overfitting patches with statically extracted code features.	2021	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9399306	41	Ye, He, et al. "Automated classification of overfitting patches with statically extracted code features." IEEE Transactions on Software Engineering 48.8 (2021): 2920-2938.	Automated program repair; patch assessment; overfitting patch; code features;	Feature extraction; Maintenance engineering; Training; Tools; Syntactics; Software; Predictive models;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	This paper introduces ODS, a method for correctness assessment of APR patches. It uses static comparison that is then fed to an ML model for classification. They train on 10k patches from Defects4J, Bugs.jar and Bears.	classifies better than other automated methods (71.9%). Their training uses project-based segmentation to avoid training on what they test (since defect sets overlap, e.g., between Defects4J and Bugs.jar)	FALSE	TRUE	TRUE	TRUE	TRUE	FALSE	TRUE	Defects4J, Bugs.jar, Bears	ODS (overfitting detection system)	https://github.com/ASSERT-KTH/ODSExperiment				I looked but did not find this information in the paper	I looked but did not find this information in the paper								FALSE	FALSE	TRUE	Random forest classifier; gradient boosting algorithm	FALSE	FALSE		unspecified	unspecified (depends on size of diffs I think)	FALSE		Patches generated in other APR experiments (several sources, including RepairThemAll)	FALSE	n/a, this is a patch quality assessment, not an APR tool			TRUE	TRUE	correctness is proxied here at times by an automatically generated test suite that is defined based on the human repair, then applied to a candidate patch as additional validation		
TSE	Chakraborty, Saikat, et al	Deep learning based vulnerability detection: Are we there yet.	2021	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/9448435	42	Chakraborty, Saikat, et al. "Deep learning based vulnerability detection: Are we there yet." IEEE Transactions on Software Engineering (2021).	Vulnerability; deep learning based vulnerability detection; real world vulnerabilities; graph neural network based vulnerability detection;	Predictive models; Neural networks; Testing; Data models; Security; Training; Training data;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	Meta-paper that looks at how well deep learning techniques perform for real vulnerability prediction (as opposed to learning features of their native datasets). Shows performance drop of more than 50%. Proposes better data collection and model design.	This paper is about identifying vulnerabilities, not repairing them. DL-fault localization, as it were.	FALSE	TRUE	TRUE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Dam, Hoa Khanh, et al	Automatic feature learning for predicting vulnerable software components.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8540022	43	Dam, Hoa Khanh, et al. "Automatic feature learning for predicting vulnerable software components." IEEE Transactions on Software Engineering 47.1 (2018): 67-85.	Software vulnerability prediction; Mining software engineering repositories; Empirical software engineering;	Semantics; Software systems; Predictive models; Security; Feature extraction; System recovery;	FALSE	FALSE	FALSE	FALSE	TRUE	Joe	LSTM model for vulnerability prediction (applied to Firefox and 18 android apps)	This paper is about identifying vulnerabilities, not repairing them. DL-fault localization, as it were.	FALSE	TRUE	TRUE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Chen, Zimin, et al	Sequencer: Sequence-to-sequence learning for end-to-end program repair.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8827954	44	Chen, Zimin, et al. "Sequencer: Sequence-to-sequence learning for end-to-end program repair." IEEE Transactions on Software Engineering 47.9 (2019): 1943-1959.	Program repair; Machine learning;	Maintenance engineering; Computer bugs; Vocabulary; Training; Natural languages; Benchmark testing;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	SequenceR is a sequence-to-sequence program repair tool, evaluated on real bug fixes and Defects4J. Correctly patches 14/75 single-line bugs in Defects4J with perfect fault localization.	Max context 1k tokens. LSTM/BiLSTM architecture. Global attention. Beam size 50	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4j	SequenceR	https://github.com/ASSERT-KTH/sequencer				1.2 hours on NVIDIA K80									FALSE	FALSE	TRUE	LSTM/BiLSTM sequence-to-sequence learning	TRUE	TRUE	linear combination of hidden encoder states (global?)	~2^21 (back of envelope from description of matrix sizes)	max 1k, usually much smaller	FALSE		Defects4J one-liners	TRUE	perfect line			TRUE	TRUE	semantic equivalence to human repair		
TSE	Liva, Giovanni, et al	Automatic repair of timestamp comparisons.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8877769	45	Liva, Giovanni, et al. "Automatic repair of timestamp comparisons." IEEE Transactions on Software Engineering 47.11 (2019): 2369-2381.	Software verification; Program verification; Formal methods; Error handling; Error recovery;	Java; Program debugging; Program diagnostics; Program testing; Public domain software; Software maintenance;	FALSE	TRUE	FALSE	TRUE	TRUE	Joe	APR for timestamp comparisons only. Uses static analysis and APR mutation methods without using test cases to validate. Correctly fixes all 246 errors of this type in their curated dataset.	This seems incredibly tightly constrained. They have impressive results on this one domain. But I wouldn't expect much of what they do to generalize past, perhaps, other numeric calculations.	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	custom, cherry-picked	TTS (time type system)	https://github.com/rtse-project/automatic-repair-time-comparison				11 hours total runtime	unspecified	11 hours total runtime across 20 projects							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		custom set of Java projects where they identified time comparison bugs	FALSE	static analysis, synthesis			FALSE	TRUE	they claim all their repairs are correct, which is probably true given the extremely limited problem domain		
TSE	Chen, Liushan, Yu Pei, and Carlo A	Contract-based program repair without the contracts: An extended study.	2020	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8972483	46	Chen, Liushan, Yu Pei, and Carlo A. Furia. "Contract-based program repair without the contracts: An extended study." IEEE Transactions on Software Engineering 47.12 (2020): 2841-2857.	Contract-based program repair; Automated program repair; Expected program behavior; Jaid; APR; Java programs; Overfitting problem; Bugs; plain Java code;	Maintenance engineering; Computer bugs; Tools; Java; Monitoring; Contracts; Programming;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	Large evaluation of Jaid, a Java repair tool evaluated on Defects4J, QuixBugs, and IntroClassJava 	Uses state abstractions for fault localization and fix generation. Results were pretty middle-of-the-road	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J, QuixBugs, IntroClassJava	Jaid	https://bitbucket.org/maxpei/jaid/wiki/Home			https://bitbucket.org/maxpei/jaid/wiki/Home		one core of a Xeon E-2630 v2, 8GB RAM, Ubuntu 14.04	not sure there was a hard limit, but highest reported time was 10 hours. Most were an hour or less.							FALSE	FALSE	FALSE		FALSE	FALSE				FALSE		Defects4J, QuixBugs, IntroClassJava	FALSE	custom SBFL		summary	TRUE	TRUE	held-out tests for IntroClassJava; semantic equivalence otherwise		
TSE	Gavidia-Calderon, Carlos, et al	The assessor's dilemma: Improving bug repair via empirical game theory.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8852726	47	Gavidia-Calderon, Carlos, et al. "The assessor's dilemma: Improving bug repair via empirical game theory." IEEE Transactions on Software Engineering 47.10 (2019): 2143-2161.	Software process; Game theory; Bug report; Priority inflation;	Task analysis; Computer bugs; Logic gates; Games; Nash equilibrium; Software;	TRUE	FALSE	FALSE	FALSE	TRUE	Joe	Not an APR paper. It introduces a game theoretic technique for managing QA engineers who label bugs as high priority to try to get their things fixed first.	Kinda funny, actually. But not relevant. It's the software engineering equivalent of that one annoying person in HR who sends all emails with Outlook's "this is important" flag set. And writes in all caps, bold, underlined, highlighted.	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Afzal, Afsoon, et al	SOSRepair: Expressive semantic search for real-world program repair.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8854217	48	Afzal, Afsoon, et al. "SOSRepair: Expressive semantic search for real-world program repair." IEEE Transactions on Software Engineering 47.10 (2019): 2162-2181.	Automated program repair; semantic code search; Patch quality; Program repair quality; SOSRepair;	Maintenance engineering; Semantic search; Encoding; Benchmark testing; Computer bugs; Software;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	SOSRepair is a C based repair tool that targets ManyBugs. It patches 22 of 65 defects (a subsetted sample)	semantic similarty metric for code replacement; ManyBugs is subsetted to be ones that run on modern VM (not Fedora 13) so not valgrind and fbc; also required to be single-hunk bugs	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	ManyBugs	SOSRepair	https://github.com/squaresLab/SOSRepair			https://github.com/squaresLab/SOSRepair-Replication-Package	115 hours (worst case) to build code database	Ubuntu 16.04 server with 16 Xeon E5-2699 v3s processors and 64 GB RAM								FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE	SBFL		per-bug	TRUE	TRUE	for IntroClass, held-out tests. For ManyBugs, semantic equivalence to human repair		
TSE	Muntean, Paul, et al	Intrepair: Informed repairing of integer overflows.	2019	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8862860	49	Muntean, Paul, et al. "Intrepair: Informed repairing of integer overflows." IEEE Transactions on Software Engineering 47.10 (2019): 2225-2241.	Program repair; Source code refactoring; Integer overflow; Software fault; Symbolic execution; Static program analysis;	Maintenance engineering; Software; Tools; Fault detection; Runtime; Engines; Fuses;	FALSE	TRUE	FALSE	FALSE	TRUE	Joe	IntRepair is focuses on integer overflows in C code. Static symbolic execution. Applied to 2052 C programs (1M LoC)	They also did a human study and found their technique helped humans repair these bugs 30x faster than the same IDE without their plugin. Deployed as an Eclipse IDE plugin	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	SAMATE's Juliet test suite	IntRepair	https://github.com/TeamVault/IntRepair				static analysis averaged less than 10 minutes for the largest programs	Dell desktop: Intel Q9550 @ 2.83 GHz, 12GB RAM, Eclipse Kepler, OpenSuse 13.01								FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE	their synthesis methods were correct by construction, so if they fixed it, it was correct		
TSE	Liu, Kui, et al	Mining fix patterns for findbugs violations.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8565907	50	Liu, Kui, et al. "Mining fix patterns for findbugs violations." IEEE Transactions on Software Engineering 47.1 (2018): 165-188.	Fix pattern; pattern mining; program repair; findbugs violation; unsupervised learning;	Tools; Static analysis; Computer bugs; Maintenance engineering; Software; Java; Security;	FALSE	FALSE	FALSE	TRUE	TRUE	Pemma	High false-positive rates of static analysis tools make resolving true positives more difficult. authors provide an approach that identifies fix patterns which can be applied to unfixed violations. the generated fixes are accepted by developers more often than not and were applicable to 4 Defects4J bugs. (pdr: this seems really low)	Not exactly APR, but extension for existing static analysis tools	FALSE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J (bugs appear cherry-picked?)				GHTorrent (curated db from github.com)		n/a	n/a	not reported							FALSE	FALSE	TRUE	CNN	TRUE	FALSE	n/a	1000 nodes in hidden layers, learning rate = 1e^-3	not reported	FALSE		Defects4J	FALSE	n/a	data preprocessing for violation tokenization outlined in paper (2.4.3); 	summary info provided	FALSE	FALSE			
TSE	Gazzola, Luca, Daniela Micucci, and Leonardo Mariani	Automatic Software Repair: A Survey,	2017	https://ieeexplore.ieee.org/document/8089448	51	Gazzola, Luca, Daniela Micucci, and Leonardo Mariani. "Automatic Software Repair: A Survey," in IEEE Transactions on Software Engineering, vol. 45, no. 1, pp. 34-67, 1 Jan. 2019, doi: 10.1109/TSE.2017.2755013.	Automatic program repair; generate and validate; search-based; semantics-driven repair; correct by construction; program synthesis; self-repairing	Software; Computer bugs; Software algorithms; Conference	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	survey paper ---- skipping		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
FSE	Xia, Chunqiu Steven, and Lingming Zhang	Less training, more repairing please: revisiting automated program repair via zero-shot learning.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/abs/10.1145/3540250.3549101	52	Xia, Chunqiu Steven, and Lingming Zhang. "Less training, more repairing please: revisiting automated program repair via zero-shot learning." Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2022.	Automated Program Repair; Deep Learning; Zero-shot Learning;	Computer systems organization; Architectures; Other architectures; Neural networks; Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	TRUE	TRUE	Pemma	The authors treat APR as a translation task and propose AlphaRepair  Issues tackled by author: (1) quality of NMT training data is suspect due to scraping open source projects and handcrafted heuristics to filter/identify commits. Can include irrelevant commits / code changes  (2) commits that form training data are limited to ones with a few changed lines. Can restrict the edit variety of approaches (3) input representations may preclude analysis of patch location within input. Can miss subtle relationships between patch and context.		TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0), Defects4J (v2.0.0), QuixBugs (Java and Python)	AlphaRepair				not provided	Yes (5hr max-time limit for each bug)	Yes (8-core workstation with Intel i7 10700KF Comet Lake CPU @3.80GHz and 16GB RAM, running Ubuntu 20.04.3 LTS and OpenJDK Java 64-Bit Server version 1.8.0_312 with NVIDIA GeForce RTX 3080 Ti GPU)					"To demonstrate the generalizability on additional projects and bugs and confirm that AlphaRepair is not simply overfitting to bugs in Defects4J 1.2, we evaluate AlphaRepair on the 82 single line bugs in Defects4J 2.0 dataset. Table 4 shows the results compared against other baselines on Defects4J 2.0. We observe AlphaRepair is able to achieve the highest number of correct patches of 36 (3.3X more than top baseline). Defects4J 2.0 contains a harder set of projects for APR with different variety of fixes compare to Defects4J 1.2. We observe that while template-based tools such as TBar was able to generate a high amount of correct patches for Defects4J 1.2, the number of correct patches it can generate for Defects4J 2.0 is limited. "		codeBERT: The model is trained on bi-modal data (documents & code) of CodeSearchNet	TRUE	TRUE	TRUE	CodeBERT	TRUE	TRUE	see CodeBERT	not reported	?	FALSE		Defects4J (v1.2.0), Defects4J (v2.0.0), QuixBugs (Java and Python)	TRUE	perfect and top-40-suspicious line-SBFL		summary, per-program	TRUE	TRUE	correct patches that correctly fix the underlying bug. authors use UniAPR for  classification  Perfect fault localization: beamwidth = 25 Top-40 suspicious lines: beamwidth = 5  		
FSE	Oh, Wonseok, and Hakjoo Oh	PyTER: effective program repair for Python type errors.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3540250.3549130	53	Oh, Wonseok, and Hakjoo Oh. "PyTER: effective program repair for Python type errors." Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2022.	Program Repair; Program Analysis; Debugging;	Computer systems organization; Embedded and cyber-physical systems; Robotics; Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	The authors present an APR technique that focuses on python type errors. Their method uses dynamic and static analyses to infer correct and incorrect types of program variables. Differences in these variables are leveraged to identify fault locations and patch candidates.	focuses on TYPE ERROR	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	Type-Bugs	PyTER	looks available		looks available	https://doi.org/10.6084/m9.figshare.20448573.v1	Yes (max-run 3,600s per bug)	Yes ((Ubuntu 18.04) with 2 CPUs and 128GB memory, powered by the Intel Zeon Silver 4214 processor)	avg time neg and pos tests, avg time running per bug							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	TRUE	Python type focused benchmark	Type-Bugs	FALSE	two stage SBFL, method then line		summary, per-program	TRUE	TRUE	We manually checked whether the generated patches are correct or not, where a patch is considered correct if it is semantically identical to a developer patch ignoring I/O side effects (e.g., printing a value). We also checked the developer’s comments to consider their implicit intention when checking semantic equivalence.		
FSE	Kim, Seulbae, and Taesoo Kim	RoboFuzz: fuzzing robotic systems over robot operating system (ROS) for finding correctness bugs.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3540250.3549164	54	Kim, Seulbae, and Taesoo Kim. "RoboFuzz: fuzzing robotic systems over robot operating system (ROS) for finding correctness bugs." Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2022.	Robot Operating System 2 (ROS 2); Correctness bugs; Semantic feedback-driven fuzzing;	Computer systems organization; Embedded and cyber-physical systems; Robotics; Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	The authors present a fuzzing method for ROS (robotics operating system) such that correctness bugs specific to robotic systems, i.e., violations of physical laws, violations of specifications, and cyber-physical discrepancies, are targeted. 		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE		RoboFuzz				not provided	Yes (12hr runtime)	Yes (Ubuntu 20.04, with Intel i7-8850H 2.6Ghz, 16GB) RAM, and Quadro P2000 mobile GPU.								FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	not applicable	manually-generated oracle ~ avg 2wk implementation 		FALSE	FALSE			
FSE	Fu, Michael, et al	VulRepair: a T5-based automated software vulnerability repair.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3540250.3549098	55	Fu, Michael, et al. "VulRepair: a T5-based automated software vulnerability repair." Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2022.	Software Vulnerability Repair;	Security and privacy; Software and its engineering; 	FALSE	TRUE	FALSE	TRUE	TRUE	Pemma	NMT-based repair system using T5 arch (Text-to-Text Transfer Transformer) with self-attention and BPE tokenization (for meaning subwords/subphrases).  The BIG IDEA from this paper: training any model with PL/NL improves overall performance in code tasks, pre training (unsupervised learning)  and BPE in architecture is critical for this arch's success. Input: vulnerable function, where each input sequence contains a special tag that specifies the CWE type of the sequence and labels vulnerable code snippet using special tags	"our VulRepair employs a pre-training CodeT5 component from a large codebase (i.e., CodeSearchNet+C/ C# [23, 56] with 8.35 million functions from 8 different Programming Languages) to generate more meaningful vector representation, employs BPE tokenization to handle Out-Of-Vocabulary (OOV) issues, and employs a T5 architecture that considers the relative position information in the self-attention mechanism"  pdr: the authors include an evaluation of their tool's ability to repair vulnerabilities by comparing tool output to "perfect prediction" (actual or human repair) for "the top-10 most dangerous CWEs".  Weird thing, after reading about CodeT5 (which contributes BPE tokenization), it seems that this research team basically just applied CodeT5 to repair tasks and did a pretty good evaluation.   "Recently, Liu et al. [30] suggested that the accuracy of an automated program repair approach should not be solely evaluated based on a perfect match. Instead, other measures should also be considered, e.g., the number of semantically correct repairs and the number of plausible patches. However, these two measures require test cases for evaluating whether the repairs can successfully pass the test cases or not. Unfortunately, there exists no test cases available in the experimental dataset that we used in this paper. Thus, both measures cannot be evaluated. Nevertheless, future researchers should create new vulnerability repair datasets where such repairs are reproducible and test case information is available."	TRUE	FALSE	TRUE	FALSE	TRUE	FALSE	FALSE	CVEFixes and Big- Vul datasets 	VulRepair	Reproduction package, including data processing, model training and evaluation is available in github @ https://github.com/awsm-research/VulRepair and https://zenodo.org/record/7080271	no trained model is available, but reuses pre-trained CodeT5 model	studied dataset is available		Yes ([fine-tuning] training time: 5 hours)	Yes (NVIDIA RTX 3090)	not reported		VRepair, CodeBERT		Same as Chen et al. [13], we split the experimental dataset into 70% of training, 10% of validation, and 20% of testing data.	We use our training dataset to fine-tune the pre-trained model to get suitable weights for our vulnerability repair task.	The CodeT5 model was pretrained on CodeSearchNet Husain et al., 2019. Additionally, the authors collected two datasets of C/CSharp from BigQuery1 to ensure that all downstream tasks have overlapped programming languages with the pre-training data. In total, around 8.35 million instances are used for pretraining. (from huggingface model card)  The Google BigQuery Public Datasets program now offers a full snapshot of the content of more than 2.8 million open source GitHub repositories in BigQuery. 	TRUE	TRUE	TRUE	T5 architecture (CodeT5 - Encoder/Decoder stacks with self-attention with relative position, ends with softmax activation at final linear layer - with Byte Pair Encoding (BPE) tokenization); Paper has a GREAT description and diagram of architecture	TRUE	TRUE	relative position self-attention (obtained via T5-reuse)	Not reported in text, but should be same as CodeT5/T5 - unfortunately, CodeT5 has two models: small (CodeT5 size not reported, but T5 is 60M) and large (770M) 	# of tokens in (vulnerable function, repaired tokens) are reported for 1st (138, 12) and 3rd quartiles (593,48), median (280,24), and average (586,55)	FALSE	n/a	CVEFixes, BigVul	TRUE	Each vulnerable code snippet in the input sequences is labeled using the special tags "<StartLoc>" and "<EndLoc>", where "<StartLoc> indicates the beginning of the vulnerable code snippet, which will be ending with the special tag "<EndLoc>".	None	 summary in paper, but zenodo ZIP contains CSV file 'rq1_cve_fixes_raw_preds.csv' 	TRUE	TRUE	"correct" exactly matches ground truth data  Calculates "% Perfect Predictions" as "total number of correct predictions divided by the total number of functions in the testing dataset".  50 repair candidates are generated for each vulnerable function in the testing dataset.  Summary of correct are reported in paper (745 / 1706)		
FSE	Dutta, Saikat, August Shi, and Sasa Misailovic	Flex: fixing flaky tests in machine learning projects by updating assertion bounds.	2021	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3468264.3468615	56	Dutta, Saikat, August Shi, and Sasa Misailovic. "Flex: fixing flaky tests in machine learning projects by updating assertion bounds." Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021.	Flaky tests; Machine Learning; Extreme Value Theory ;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	The authors present a method that focuses on improving flaky tests that occur due to randomness.	This is focused more on test improvement, not program repair, skipping	FALSE	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
FSE	Wong, Chu-Pan, et al	VarFix: balancing edit expressiveness and search effectiveness in automated program repair.	2021	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3468264.3468600	57	Wong, Chu-Pan, et al. "VarFix: balancing edit expressiveness and search effectiveness in automated program repair." Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021.	automatic program repair; variational execution;	Software and its engineering; Software creation and management; Software development techniques; Error handling and recovery; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	The authors present a method that focuses on improving search effectiveness by reducing test execution through "variational execution" (this executes a program for multiple inputs/variants at one time, splitting execution with and without the edit, merging execution.	This tool augments existing APR tools, such that it outputs a set of plausible patches, filtered by minimized patch edits and ranked	TRUE	FALSE	FALSE	TRUE	TRUE	FALSE	TRUE	Defects4J, IntroClass	VarFix				https://chupanw.github.io/varfix-supplement/	Yes (the experiments for RQ1 took more than 5000 hours of CPU time)	Yes (2 vCPU and 16 GB of RAM)								FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J, IntroClass	FALSE	"We first use GenProg’s fault localization technique to narrow down where to generate edits."		summary, per-program	TRUE	TRUE	"we can verify patch correctness formally via symbolic execution for most of the bugs"  "Another common strategy is to evaluate patches with held-out tests [15, 28, 38], which we adopt in this work for patches we cannot formally verify against a reference implementation."		
FSE	Yang, Chen	Accelerating redundancy-based program repair via code representation learning and adaptive patch filtering.	2021	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3468264.3473496	58	Yang, Chen. "Accelerating redundancy-based program repair via code representation learning and adaptive patch filtering." Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021.	representation learning; patch filtering; automated program repair;	Software and its engineering; Software notations and tools; Software maintenance tools;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	The authors introduce these terms: "redundancy-based" to refer to APR tools that search for code snippets similar to faulty code base as references, then generate a number of candidate patches. (they include GenProg in this!?!?!) "inaccurate similarity problem" generating too many incorrect patches and wasting time validating them "(patch) order problem" the order that patches are evaluated can waste time when incorrect patches precede correct ones.   patches are ordered by similarity to original faulty statement  this is a preliminary study 	This is a preliminary study, The tool augments existing APR tools, to screen out incorrect patches and rank patches for evaluation	TRUE	FALSE	TRUE	TRUE	FALSE	FALSE	FALSE		AccPR				not provided										FALSE	FALSE	TRUE	ASTNN	TRUE	FALSE	n/a	not reported	not reported	FALSE			TRUE	statement-level perfect		per-bug, summary	FALSE	FALSE	not defined		
FSE	Song, Dowon, Woosuk Lee, and Hakjoo Oh	Context-aware and data-driven feedback generation for programming assignments.	2021	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3468264.3468598	59	Song, Dowon, Woosuk Lee, and Hakjoo Oh. "Context-aware and data-driven feedback generation for programming assignments." Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021.	Program Repair; Program Synthesis;	Software and its engineering; Software creation and management; Software development techniques; Automatic programming; 	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	The authors present a method for generating feedback for programming assignments automatically, particularly as personalized assistance for programming education. Their method uses a context-aware function level repair algorithm, leveraging multiple partial references to match a solution to a buggy submission. 		TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE		Cafe				not provided	Yes (max time-limit, 60s per program)	Yes (iMac w/ Intel i5 CPU and 16GB mem)	not provided							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	uses function call contexts to perform fault localization, similar to DUA or Calltree analysis with code context		summary	TRUE	TRUE	plausible rate is reported (manually evaluated) test cases used as correctness specifications		
FSE	Zhu, Qihao, et al	A syntax-guided edit decoder for neural program repair.	2021	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3468264.3468544	60	Zhu, Qihao, et al. "A syntax-guided edit decoder for neural program repair." Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021.	Automated program repair; Neural networks;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	Authors indicate that their approach fixes 53 bugs on v1.2.0, 11 bugs more than the SOA repair mechanism, TBar. The authors methodology focus on addressing these limitations to translation-based DL approaches: (1) syntactically incorrect patches are generated (2) the number of tokens from the input effectively outlines the size of the program space with these tokens (this is not so clear why this is a problem). if you consider only edit space, rather than token/translation space, the search space is smaller (ehh, maybe revisit this after reading the rest of the paper) (3) identifiers that aren't present in the program are introduced in repairs  Authors claim that this is the first DL-NPR method that outperforms traditional APR algorithms 		TRUE	TRUE	TRUE	FALSE	TRUE	TRUE	TRUE	Defects4J (v1.2.0), Defect4J (v2.0.0), IntroClass, QuixBugs	Recoder	yes			https://github.com/pkuzqh/Recoder (includes source code for tool, generated patches, and demo)	Yes (5 hour max run limit, no other time is reported)	not reported	not provided							FALSE	FALSE	TRUE	TreeGen [64], a tree-based Transformer [67] that takes a natural language description as input and produces a program as output.  No attention model outlined	TRUE	TRUE	not reported	?	?	FALSE		Defects4J, IntroClass, QuixBugs 	TRUE	Both Perfect and SBFL (Ochiai via GZoltar) 		summary, per-bug available in replication package	TRUE	TRUE	two authors have independently checked the correctness of the patches, and a patch is considered correct only if both authors consider it correct. The generated patches also have been released for public assessment.		
FSE	Xiao, Ya	Multi-location cryptographic code repair with neural-network-based methodologies.	2021	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3468264.3473102	61	Xiao, Ya. "Multi-location cryptographic code repair with neural-network-based methodologies." Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021.	cryptographic API misuse; neural networks; code embedding; code suggestion;	Security and privacy; Software and application security;	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	PHD Thesis - not full paper The authors presents this PHD thesis proposal that focuses on cryptographic API misuses that occurs in software programs. The authors propose a neural-network-based architecture, but no details on evaluation are provided.	not full paper, skipped after summary	FALSE	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
FSE	Koyuncu, Anil, et al	iFixR: Bug report driven program repair.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3338906.3338935	62	Koyuncu, Anil, et al. "iFixR: Bug report driven program repair." Proceedings of the 2019 27th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering. 2019.	Information retrieval; fault localization; automatic patch generation;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper investigates the feasibility of APR systems that use bug reports, and propose a program repair tool that uses information retrieval (IR)-based fault localization with fix patterns to identify repair candidates. Generated patches are prioritized for developer review.		TRUE	FALSE	FALSE	FALSE	TRUE	TRUE	TRUE	Defects4J (395 bugs)	iFixR	looks available		looks available	https://github.com/SerVal-DTF/iFixR	not reported	not reported								FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J (395)	FALSE	uses user-generated bug reports for fault localization	Bug reports	summary; per program info	TRUE	TRUE	"correct" not specifically defined, but references "actually addressing the bug"		
FSE	Bavishi, Rohan, Hiroaki Yoshida, and Mukul R	Phoenix: Automated data-driven synthesis of repairs for static analysis violations.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3338906.3338952	63	Bavishi, Rohan, Hiroaki Yoshida, and Mukul R. Prasad. "Phoenix: Automated data-driven synthesis of repairs for static analysis violations." Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2019.	program synthesis; program repair; static analysis; programming-by-example;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; Software notations and tools; Context specific languages; Domain specific languages; Programming by example; Software organization and properties; Software functional properties; Formal methods; Automated static analysis;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper focuses on automatically generating patches for static analysis violations. The authors use static analyzers as an oracle (no test suite required).  The authors define a domain-specific language that is used to represent their repair strategies, which consiste of a name and rule that describes the transformation to be applied.  A rule consists of a context-matcher to locate an AST node (a primary node) and a list of fixes.	uhh, the author basically constructs DFAs of the edits, combining similar DFAs, then uses each DFA as a graph and employing Dijkstra's algorithm with cost metric as function of the length, edges, and context-matchers required in the visitor. The log(n)-optimal greedy approximate algorithm for set-cover can be used to find the least-cost subset of context-matcher predicates. 	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	From GHTorrent, 517 un-forked Java projects with at least 500 commits and use Maven, and such that FindBugs can execute on them. 	Phoenix	cant find this		available in replication package	https://figshare.com/s/8ba50b84deee6a826ced	Yes (40s on avg. to learn strategies from all examples, 50s on avg. to fully process a violation and generate a ranked list of patch suggestions.)	Yes (two 64-core machines, each with Intel Xeon 2.60 GHz processors with 128GB of memory running Ubuntu 16.04 LTS 64-bit; 8-core machine with Intel i7-4790 3.60GHz CPU, 16GB of memory running Ubuntu 16.04 LTS 64-bit)								FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE				FALSE	TRUE	"correct" means fixes the violation		
FSE	Ginelli, Davide	Failure-driven program repair.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3338906.3341464	64	Ginelli, Davide. "Failure-driven program repair." Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2019.	Automatic program repair; software defects; automatic debugging;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging;	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	PHD thesis - not full paper	Skipping	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
FSE	Hua, Jinru, et al	Sketchfix: a tool for automated program repair approach using lazy candidate generation.	2018	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3236024.3264600	65	Hua, Jinru, et al. "Sketchfix: a tool for automated program repair approach using lazy candidate generation." Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2018.	Program Repair; Program Synthesis; Program Sketching;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper tackles inefficiencies in the generate-and-validate approach used by a number of APR tools, specifically the number of recompilation and re-executions of the program. One approach, it reduces repair locations based on run-time information. 	I don't get the big deal here.	TRUE	FALSE	FALSE	FALSE	TRUE	TRUE	TRUE	Defects4J	SketchFix					Yes (on average, spends 9 minutes to locate faults and generate sketches, and 23 minutes to generate the first repairs that satisfy all test assertions)	not reported	not reported							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J (357 bugs)	FALSE	Statement-level SBFL, Ochiai		for 26 repairs only -- summary; per-bug ; not sure where the rest of the benchmark results are	TRUE	TRUE	"correct" is not well-defined in this paper, but seems to allude to repairs that pass all tests and pass manual inspection -- manual inspection methodology is not defined		
FSE	Yang, Junwen, et al	Powerstation: Automatically detecting and fixing inefficiencies of database-backed web applications in ide.	2018	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3236024.3264589	66	Yang, Junwen, et al. "Powerstation: Automatically detecting and fixing inefficiencies of database-backed web applications in ide." Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2018.	performance anti-patterns; Object-Relational Mapping frameworks; database-backed applications; RubyMine Plugin;	Software and its engineering; Software creation and management; Software post-development issues; Maintaining software; Software organization and properties; Extra-functional properties; Software performance;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	The authors present a method that uses automated static analysis to identify and suggest resolutions for ORM (object relational mapping) framework-related issues. ORM allow web-application developers to build web applications in an object oriented manner. Some example ORMs are Ruby on Rails, Django, Hibernate. Their platform 'PowerStation' is an IDE plugin that detects 6 common performance anti-patterns and generates patches for 5 of them.	What is an anti-pattern? per Wikipedia: "An anti-pattern in software engineering, project management, and business processes is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive."  While this is APR adjacent, in performance land, not exactly APR, but skipping rest	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	12 open-source Rails applications	PowerStation				www.hyperloop.cs.uchicago.edu/powerstation/	Yes (12-635 s to analyze entire applications)	Yes (Chameleon instance with 128GB RAM + 2 CPUs)								FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE				FALSE	FALSE	no false positives were identified in half of the suggested fixes randomly sampled		
ASE	Dissanayake, Nesara, et al	An Empirical Study of Automation in Software Security Patch Management.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3551349.3556969	67	Dissanayake, Nesara, et al. "An Empirical Study of Automation in Software Security Patch Management." 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.	security updates; patch management; vulnerability management;	Security and privacy; Software and application security; Software security engineering; Systems security; Vulnerability management; Software and its engineering; Software creation and management; Software post-development issues; 	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper focuses on how automation can benefit managing and deploying security patches.	Not APR, but timely patching of known vulnerabilities => skipping	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ASE	Zhang, Chen, et al	BuildSonic: Detecting and Repairing Performance-Related Configuration Smells for Continuous Integration Builds.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3551349.3556923	68	Zhang, Chen, et al. "BuildSonic: Detecting and Repairing Performance-Related Configuration Smells for Continuous Integration Builds." 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.	continuous integration; configuration smells; build performance;	Software and its engineering; Software notations and tools; Software configuration management and version control systems; Software organization and properties; Extra-functional properties; Software performance; 	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper focuses on optimizing continuous integration builds such that build performance can improve. They idnentify "performance-related CI configuration smells" or PCs from three CI tools (Travis, Maven, Gradle) for Java projects. 	NOT APR, but CI Build efficiency => skipping	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE		BuildSonic														FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
ASE	Tian, Haoye, et al	Is this Change the Answer to that Problem? Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3551349.3556914	69	Tian, Haoye, et al. "Is this Change the Answer to that Problem? Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness." 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.	Patch Correctness; Program Repair; Question Answering; Machine Learning;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This approach to patch correctness employs Question-Answering techniques. First, a description of the bug is extracted from bug reports.		FALSE	TRUE	TRUE	TRUE	FALSE	FALSE	TRUE	Defects4J, Bugs.jar, Bears	Quatrain	Looks available			https://github.com/Trustworthy-Software/Quatrain					CodeTrans-TF-Large	[Insufficient deduplication of semantically-equivalent patches may lead to biased prediction performance.] As we mentioned in the experimental design in Section 5.1, the traditional 10-fold cross validation scheme may assign the same semantically-equivalent patches simultaneously into both train and test datasets. In practice, this setup violates the principles in machine/deep learning-based evaluations since it’s equivalent to letting the models cheat by learning knowledge from test data during the training process[1, 17 , 65 ]. To showcase this bias in the results, we propose to focus on a straightforward classifier using a random forest on the embeddings of the bug report and the patch: when using 10-fold cross validation scheme on our ground truth dataset, the achieved AUC is as high as 0.978 (with F1 at 0.860); however, when using our deduplication scheme (10-group cross validation based on bug ID), the AUC drops to 0.780 (and F1 at 0.344).			pre-trained deep learning model BERT  NOT FOR APR COMPONENT	TRUE	TRUE	TRUE	CodeTrans-TF-Large	TRUE	TRUE	Attention from Tan "LSTM-based deep learning models for non-factoid answer selection."	not reported, but max sequence length == 64, hidden state dim == 15	difficult to determine from text	FALSE		Defects4J (v? includes Closure-96?), Bugs.jar, Bears	FALSE	obtained from patch changes			FALSE	TRUE	A "correct" patch implements changes that "answer" to a problem posed by buggy behavior		
ASE	Ye, He, et al	SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3551349.3556926	70	Ye, He, et al. "SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics." 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.		Computing methodologies; Machine learning; Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; Software notations and tools; Software configuration management and version control systems; Theory of computation;	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	The authors' approach basically highly train a NN model on perturbed versions of a faulty program, rather than using mined commits. They also encode execution details of the variant into the input representation.		TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J 	SelfAPR	Looks available	not available	looks available	https://github.com/SophieHYe/SelfAPR	Not reported	Not reported	not reported; but beam is 50			Sanity check of perturbation-based training samples. All training samples are validated with the following sanity check: 1) they are different from the correct program; 2) they are unique, i.e., we deduplicate the training samples even if different 𝑅𝑢𝑙𝑒𝑠 generate the same training samples; 3) they are buggy. Then, we guarantee that the perturbed code indeed triggers a bug by executing them against the compiler and test suite.	We evaluate our work on 818 bugs from 17 open-source projects from Defects4J [ 32 ]. In total, we generate 1 039 873 training samples in a self-supervised manner, each of which contains an error diagnostic. Our experimental results show that SelfAPR  succeeds in repairing 65/388 bugs from Defects4J version 1.2 and 45/430 bugs from Defects4J version 2.0, which is a clear improvement over the state-of-the-art. 		All those training samples are obtained by perturbing 17 open-source projects with 327 194 lines of source code and specified with 25 812 test cases in total. Notably, the testing set is composed of all bugs from Defects4J version 2.0 minus the bugs used for training. This gives 818 (835 - 17) testing samples.	FALSE	TRUE	TRUE	transformer-based (hugging-face T5 transformer)	TRUE	TRUE		not provided	384 input tokens from buggy and context code (max output 76 tokens); vocab size of 32,128	FALSE		Defects4J (v2.0.0)	FALSE	Line SBFL (GZoltar, ochiai)	Runtime information is encoded into input; sentence-piece tokenization; 	summary; per-bug available in replication	TRUE	TRUE	"plausible" - compiles and passes developer test suite "correct" - generated/held-out tests + ( identical to developer patch or considered correct by manual evaluation [performed by two authors] )		
ASE	Zhong, Wenkang, et al	StandUp4NPR: Standardizing SetUp for Empirically Comparing Neural Program Repair Systems.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3551349.3556943	71	Zhong, Wenkang, et al. "StandUp4NPR: Standardizing SetUp for Empirically Comparing Neural Program Repair Systems." 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.	neural program repair; dataset; empirical study;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	TRUE	TRUE	TRUE	Pemma	Authors have identified a major threat to neural program repair (NPR) systems validity: they are difficult to compare as (1) have very different training data; (2) have inconsistent evaluation data; and (3) each generate a wide-range of candidates per bug.  They focus on these aspects of NPR systems: Repairability (how many bugs can be fixed by the system and how does the number of candidates generated by a system influence this?); Inclination (does the training data influence what bugs are fixed by the system? does the system tend to repair specific bug types?); Generalizability (can systems repair bugs that it hasn't seen in training?). 	The authors also focus on Fix-Rate (i.e., how many of the generated candidates actually repair the bug), as tool evaluations may not disclose this or when they do, this number may not be consistent across tools/field. INTERESTING: Authors' analysis finds that NPR systems are good at dealing with "code-delete bugs";  Main contribution: Framework to compare Neural Program Repair models for repairability, inclination, and generalizability. ==> Framework includes PREPROCESSOR PHASE that allows for preprocessing of input context as required for each tool. Authors only talk about time-cost for evaluating patch candidates identified by systems and not about the process of generating patch candidates.  AUTHORS DO NOT SCREEN OUT TEST SET WHEN THE BUGGY LINE-TO-FIX HAS A HIGH SIMILARITY TO TRAINING DATA (i.e., the line existed in some other training data's context/bug-fix)!  The authors argue that this is not a data leakage problem, because the "input-output pairs" between evaluation and training are distinct. I would argue that they don't know if it's actually data leakage.  (See Finding 7 from paper) "NPR systems have a higher probability to fix one bug if it has similar samples in the training data ("similar" refers to the similarity between bug lines). Taking a left-to-right view over Table 6, we observe that NPR systems always reach a relatively higher fix rate on bugs that have a higher similarity with the nearest sample in the training set. Specifically, when the NPR system has seen the identical buggy line during training (the similarity is equal to 1), the bug has a much more higher rate to be fixed correctly."	FALSE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	NPR4J-Benchmark	NPR4J-Benchmark	n/a	n/a	n/a		Not reported	Not reported	not reported		Tufano, SequenceR, CoCoNut, CODIT, Edits, Recoder					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	TRUE	Consists of training set (~144.6k), validation set (~13.7k), and evaluation set (~13.7k) - Java bug-fix pairs.		FALSE			not reported	TRUE	TRUE	two "correct" definitions: (1) identical to human-written patches  (2) it can pass the corresponding test cases and is checked by at least two of the authors  (1) - used in evaluation "diversity" (1) + (2) - used in "empirical" evaluation  Plausible patch generation for "diversity" evaluation is not reported.  100 patches are generated by the NMT models; however, each patch's class "plausible" / "correct" isn't validated until after generation completed.		
ASE	Yang, Deheng, et al	TransplantFix: Graph Differencing-based Code Transplantation for Automated Program Repair.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3551349.3556893	72	Yang, Deheng, et al. "TransplantFix: Graph Differencing-based Code Transplantation for Automated Program Repair." 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.	Automated program repair; graph differencing; code transplantation;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	TRUE	TRUE	Pemma	APR technique that leverages graph differencing-based transplantation from "donor method".  Novelty: (1) graph-based diff alg; (2) inheritance-hierarchy-aware code search for donor identification; (3) namespace transfer to adapt donor code to repair	Seems a little like CodePhage, but uses graph differencing	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0, v2.0.0)	TransplantFix	looks available	not applicable		https://github.com/DehengYang/TransplantFix	Yes (2 hr max-limit run-time per bug repair)	not reported	max-time budget: 2 hours for each "repair trial" (repair trial seems to indicate single APR tool invocation)							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J (v1.2.0), Defects4J (v2.0.0)	FALSE	Both FL and Perfect-method-level FL were evaluated FL: method-level fault localization using GZoltar (Ochiai) Perfect FL at method level		summary, per-bug info available in link from replication package	FALSE	TRUE	"correct" - For plausible patches that pass the test suite, we manually assess the correctness of these patches by checking if they are identical or semantically equivalent to the human-written patches.		
ASE	Li, Xueyang, et al	TransRepair: Context-aware Program Repair for Compilation Errors.	2022	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3551349.3560422	73	Li, Xueyang, et al. "TransRepair: Context-aware Program Repair for Compilation Errors." 37th IEEE/ACM International Conference on Automated Software Engineering. 2022.	Program repair; compilation error; deep learning; context-aware;	Computing methodologies; Artificial intelligence; Natural language processing; Machine translation; Software and its engineering; Software creation and management; Software development techniques; Automatic programming; Software verification and validation; Software defect analysis; 	FALSE	TRUE	FALSE	TRUE	TRUE	Pemma	The authors focus on automatically fixing compilation errors in C programs.		TRUE	TRUE	TRUE	FALSE	FALSE	FALSE	FALSE	new dataset crafted from DeepFix*, StackOverflow, TRACER*; errors were synthetically crafted (errors were induced); deduplication 	TransRepair	looks available		looks available	https://sites.google.com/view/transrepair/  Code and dataset are available in ZIP file, but contents not directly observable otherwise	Yes (around 30 hours training)	Yes (a Intel(R) Xeon(R) server with 8 cores, which equips Nvidia 3090 with 24G) memory and 2 Nvidia TITAN X with 12G memory	not reported		DrRepair, DeepFix*, RLAssist*, SampleFix*, MACER* 					FALSE	FALSE	TRUE	Transformer-based (encoder, fully-connected forward NN (MLP), pointer-decoder)	TRUE	TRUE	uses previous word's embedding and current hidden states to compute attention distribution		word-dimension is set to 256	FALSE		TRACER, DeepFix	FALSE	Uses a ML-model to determine fault localization (function provided as input)		summary, (per-bug info does not look available)	FALSE	TRUE	beam-size: variant, 5 in DrRepair comparison "correct" - programs are corrupted by a tool, compare the generated fix to original program		
ASE	Alotaibi, Ali S	Automated repair of size-based inaccessibility issues in mobile applications.	2021	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE51524.2021.9678625	74	Alotaibi, Ali S., Paul T. Chiou, and William GJ Halfond. "Automated repair of size-based inaccessibility issues in mobile applications." 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021.			FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper introduces the term "Size-based inaccessibility issues" which refers to classes of software issues related to usability issues that occur in user interfaces when they are scaled up. For example, people with disabilities, motor impairments, or simply older people can imprecisely interact with touchscreen UIs, leading to "imprecise touches, increased touch mistakes, or even the inability to access important functionalities in mobile apps." The authors indicate that the size of touch targets in touchscreen UIs are the primary cause of these aforementioned issues.	Uhhhhh, not exactly program repair, but going to see if there is any related evaluation or ML content	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	1K Google Play applications	SALEM														FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE				FALSE	FALSE			
ASE	Wang, Shangwen, et al	Automated patch correctness assessment: How far are we?.	2020	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3324884.3416590	75	Wang, Shangwen, et al. "Automated patch correctness assessment: How far are we?." Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. 2020.	Patch correctness; Program repair; Empirical assessment;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper focuses on the overfitting problem specifically by systematically evaluating automated patch correctness techniques. The authors find that syntactical and semantic code features are generally effective for classifying patches as correct or overfit.		FALSE	FALSE	FALSE	TRUE	TRUE	FALSE	TRUE	Defects4J										Randoop, Evosuite, DiffTGen, Daikon, Opad, PATCH-SIM, Anti-patterns, 					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J (202 of 395 bugs)	FALSE			not applicable	FALSE	FALSE	"correct" -- intended program specification or program behavior expected by developers		
ASE	Tian, Haoye, et al	Evaluating representation learning of code changes for predicting patch correctness in program repair.	2020	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3324884.3416532	76	Tian, Haoye, et al. "Evaluating representation learning of code changes for predicting patch correctness in program repair." Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. 2020.	Program Repair; Patch Correctness; Distributed Representation Learning; Machine learning; Embeddings;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper focuses on learning code representations which can encode features usable for patch correctness prediction. The authors leverage both pre-trained and "re-trained" neural networks for their evaluations, specifically a BERT transformer-based model. 	What is an "AUC value"? What is "Patch-Sim" and why are the authors comparing against it? From "On the efficiency of test suite based program repair": Overall our systematic study of patch generation efficiency reveals that (1) efficiency is not yet a widely-valued performance target; (2) state-of-the-art can avoid generating nonsensical patches; (3) the more templates an APR system considers, the more nonsensical and in-plausible patches it will generate; (4) specialized templates increase APR tool efficiency; and (5) correct patches are sparse in the search space.	FALSE	TRUE	TRUE	TRUE	TRUE	FALSE	TRUE	Defects4J, Bears, Bugs.jar, QuixBugs, ..., Patch-Sim		Yes	dependent upon base model	Yes	https://github.com/SerVal-DTF/DL4PatchCorrectness	not reported	not reported			BERT, Doc2Vec, code2vec, CC2Vec	Eventually, our dataset shown in Table 7 included 1000 patches after removing duplicates to avoid data bias.  Logistic Regression with BERT embeddings yielded very promising performance on patch correctness prediction with metrics like F- Measure at 0.72% and AUC at 0.8% on a labeled deduplicated dataset of 1000  patches			We use pre-trained models of  BERT (trained with natural language text) - We then train a large model (12-layer to 24-layer Transformer) on a large corpus (Wikipedia + BookCorpus) for a long time (1M update steps), and that's BERT.[https://github.com/google-research/bert/blob/master/README.md]  and CC2Vec (trained with code changes)  We used the original dataset of Jiang et al. [26 "Automatically generating commit messages from diffs using neural machine translation."] and the cleaned dataset of Liu et al. [39 "Neural-machine-translation-based commit message generation: how far are we?."] for evaluation. While the original dataset consists of a training dataset of 30K patches and a testing dataset of 3K patches, the cleaned dataset consists of a training dataset of 22K patches and a testing dataset of 2.5K patches. [https://arxiv.org/pdf/2003.05620.pdf]  as well as a retrained model of Doc2Vec (trained with patches) we have trained the Doc2Vec model with code data of 36,364 patches from the 5 repair benchmarks (cf. Table 1). 	FALSE	TRUE	TRUE	BERT (pre-trained on Wikipedia, 24-layer), Doc2Vec (trained on code data from subset of datasets identified in evaluation), code2vec(pre-trained model from java code projects), CC2Vec (pre-trained model)	TRUE	TRUE	see reused ML models	see reused ML models	line (size not specified)	FALSE		Bears, Bugs.jar, Defects4J, ManySStubBs, QuixBugs, RepairThemAll, 	FALSE	Perfect line	multi-line changes flattened into single-line; input tokenization based on ML model	summary (can't find per-bug in replication package)	FALSE	TRUE	"correct" -- intended program specification or program behavior expected by developers; however, they also deem patches incorrect when they are not within the same file or at the same code location in that same file; in the incorrect vs correct evaluation, authors assume all generated patches are plausible and use the term "incorrect" in lieu of plausible, but rely upon external tools' evaluation infrastructure for the patch generation.		
ASE	Benton, Samuel, et al	On the effectiveness of unified debugging: An extensive study on 16 program repair systems.	2020	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3324884.3416566	77	Benton, Samuel, et al. "On the effectiveness of unified debugging: An extensive study on 16 program repair systems." Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. 2020.	Unified debugging; Program repair; Fault localization;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper aims to apply "unified debugging" approach to APR, amplifying fault localization with APR results.  It evaluates the unified debugging method using 16 APR systems: 		TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (V1.0.0)	UniDebug++	Yes	not applicable	Yes	https://github.com/ProdigyXable/ UnifiedDebuggingStudy	not reported	not reported	not reported		ProFL, ACS, Dynamoth, Arja, GenProg-A, jGenProg, jKali, jMutRepair, Kali-A, RSRepair-A, Simfix, AVATAR, FixMiner, kPar, PraPR, TBAR					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J	FALSE	method-level SBFL (Ochiai)		summary (can't find per-bug in replication package)	FALSE	FALSE	"correct patch provided by developers"		
ASE	Ding, Yangruibo, et al	Patching as translation: the data and the metaphor.	2020	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3324884.3416587	78	Ding, Yangruibo, et al. "Patching as translation: the data and the metaphor." Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering. 2020.	neural machine translation; big code; sequence-to-sequence model; automated program repair;	Software and its engineering; Software notations and tools; Software maintenance tools; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper explores the subtle differences between sequence-to-sequence and translation models when applied to software engineering tasks.  The authors present a principled approach to model design for bug repair, using findings from empirical studies and general knowledge of software engineering.  PDR NOTE: The authors focus on one-line bug fixes and note that most of the tokens are preserved in fix from the original buggy line -- is this simply the result of this choice of focusing on one-liners?	This paper is a bit of a feasibility study of program repair as a translation task.  The authors focus on seq2seq models for program repair. "We collect our bugs from the history of the 10,235 most-starred Java repositories on Github on March 30th, 2020. We analyzed each project's entire commit history and extracted any commits that altered precisely a single line in a single Java file, disregarding any (spurious) changes to whitespace. We then compared the corresponding commit messages against a relatively simple keyword-based check to heuristically find commits labeled as e.g., "fix" or "bug". ... This process resulted in ~60k bug fixes across 8,644 projects in our dataset."	TRUE	TRUE	TRUE	FALSE	FALSE	FALSE	FALSE	60k bug-fixes from 8,644 screened Java Github open-source projects		Yes	not available	Yes	https://github.com/ARiSE-Lab/Patch-as-translation	not reported	not reported								FALSE	FALSE	TRUE	"vanilla Transformer" model	TRUE	TRUE	references "attention is all you need" wrt transformer		impact of context-size is studied in this evaluation	FALSE		GitHub open-source Java projects	TRUE	perfect line; variable context size	authors apply BPE (byte-pair-encoding) as subtokenization mechanism	summary, percentage top-[1,5,25]	FALSE	FALSE	not defined in paper, but from context, the authors use the developer provided patch as the expected repair		
ASE	Hu, Yang, et al	Re-factoring based program repair applied to programming assignments.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE.2019.00044	79	Hu, Yang, et al. "Re-factoring based program repair applied to programming assignments." 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019.	Program Repair; Programming Education; Software Refactoring;		FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper presents an automated approach to generate repairs for student/novice programs in "real-time". Their approach requires a number of pre-existing correct solutions, which are refactored to some semantically-equivalent solution. When a failing input is provided as input, its refactoring is compared to those correct solutions in order to find a version that most closely matches its control flow structure.		TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	student submissions from an intro Python course (NUS)	Refactory									Clara					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	CFG alignment w/ known good CFGs		summary, subgroup summaries	FALSE	FALSE	not applicable ("correct" refers to suite of existing known correct solutions to programming assignment)		
ASE	Endres, Madeline, et al	Infix: Automatically repairing novice program inputs.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE.2019.00045	80	Endres, Madeline, et al. "Infix: Automatically repairing novice program inputs." 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019.	input repair; novice programs; human study;		FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper presents an approach to automatically fix program inputs, primarily targeting errors produced by novice programmers. Their implementation, InFix, focuses on repairing Python program. ~26K unique scenarios from Python Tutor, an online programming tutoring environment, are used to evaluate their implementation.		TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	custom benchmark crafted from Python Tutor	InFix					Yes (median, average runtime costs reported)	Yes (Ubuntu 18.04.2 LTS, 4.3GHz Intel i7-7740X quad-core CPU w/ 32GB mem)	not use case							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		scenarios from Python Tutor	FALSE	Error Message (python)		summary, subgroup summaries	FALSE	FALSE	not applicable		
ASE	Cashin, Padraic, et al	Understanding automatically-generated patches through symbolic invariant differences.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE.2019.00046	81	Cashin, Padraic, et al. "Understanding automatically-generated patches through symbolic invariant differences." 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019.		Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	Using program invariants, program patches can be categorically classified into semantically similar clusters, reducing the effort required to evaluate patches (from all patches to unique clusters)		FALSE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	ManyBugs (5), Defects4J (7)	PatchPart					Not reported	Not reported	not reported		Daikon					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE				FALSE	FALSE	not applicable		
ASE	Ghanbari, Ali, and Lingming Zhang	PraPR: Practical program repair via bytecode mutation.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE.2019.00116	82	Ghanbari, Ali, and Lingming Zhang. "PraPR: Practical program repair via bytecode mutation." 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019.	Program Repair; JVM Bytecode; Mutation Testing;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	Tool demonstration for "Toward practical automatic program repair." ...	Uses decompiled versions to situate repair in context of original source code.	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0, v1.4.0)	PraPR	Yes			https://github.com/prapr/prapr			not reported							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	JVM-instruction SBFL (Ochiai)		summary	FALSE	FALSE	"correct" patches are equivalent to developer patches		
ASE	Soto, Mauricio	Improving patch quality by enhancing key components of automatic program repair.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE.2019.00147	83	Soto, Mauricio. "Improving patch quality by enhancing key components of automatic program repair." 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019.	Automatic Program Repair; Patch Quality;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging; 	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	Not full paper: proposes improving test suite quality, mutation operator selection, and patch consolidation improves quality of plausible patches.		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE				FALSE	FALSE	"incorrect" does not generalize to an oracle evaluation (e.g., a knowledgeable developer or a held-out independently created test suite)		
ASE	Sharma, Vaibhav	Automatically repairing binary programs using adapter synthesis.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE.2019.00149	84	Sharma, Vaibhav. "Automatically repairing binary programs using adapter synthesis." 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019.	automated program repair; adapter synthesis; binary analysis; symbolic execution;		TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	Not full paper: Uses "adapter synthesis" with code transplantation-like mechanism	automated binary repair via "adapter synthesis"; this is a little all over the place, they mention binary repair, but they're performing symbolic analysis/synthesis at source-level in their motivation.  Seems similar to using clone/patch detection with adaptation like CodePhage...and there is a reference to CodePhage .. and SCRepair.	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	DARPA-CGC						Not reported	Not reported								FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	not reported			FALSE	FALSE	"The correct fix should pass all available tests."		
ASE	Ghanbari, Ali	Toward practical automatic program repair.	2019	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1109/ASE.2019.00156	85	Ghanbari, Ali. "Toward practical automatic program repair." 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019.	Program Repair; JVM Bytecode; Mutation Testing;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging;	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	Not full paper: The authors present a "general APR" tool that operates at the level of JVM bytecode and employs 44 mutators (18 traditional, 12 replacement, 14 check insertion).	authors subdivide APR into "dynamic repair" and "generate and validate", weird. ant the authors are totally high on their own supply? PraPR success rate on Defects4J v1.2.0 - 43 / 395, v1.4.0 deltas - 12 / 192;  Author discusses Schulte's 2010 paper as "possibility to evolve assembly code", but neglects to cite the later paper that implements it, then claims their tool is the first APR technique at the bytecode level. BOOOOO BOOOOOOO BOOOOO	FALSE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0, v1.4.0)	PraPR	Yes			https://github.com/prapr/prapr	Not reported	Not reported	not reported							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	not reported		summary	FALSE	FALSE	"correct" - semantically equivalent to developer patches		
ASE	Lin, Huarui, et al	Pfix: fixing concurrency bugs based on memory access patterns.	2018	https://dl-acm-org.ezproxy1.lib.asu.edu/doi/10.1145/3238147.3238198	86	Lin, Huarui, et al. "Pfix: fixing concurrency bugs based on memory access patterns." Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. 2018.	Multi-threading; Concurrency bugs; Memory-access pattern; Locking policy; Automatic fixing;	Software and its engineering; Software creation and management; Software verification and validation; Software defect analysis; Software testing and debugging;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	Heisenbugs are difficult to fix, and may be the result of specific schedule that results in the problematic set of locking events. The authors infer locking policies from failure-inducing memory-access patterns, speculate what locking pattern was intended, then implement locking pattern such that failing memory-access pattern is rendered impossible. The authors' system PFix is able to fix 19 of 23 concurrency Java bugs that previous SOA tool (Grail) could only fix 3.		TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	Java programs from {SIR repository, Pecan, JaConTeBe}		Yes		Yes	https://github.com/PFixConcurrency/Fix  https://github.com/PFixConcurrency/FixExamples 	Yes (runtime per bug reported in Table 3)	Yes (CPU 3.4GHz w/ 16GB mem)			Grail					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		SIR, Pecan, JaConTeBe	FALSE			per bug info	FALSE	FALSE	"correct" - manual inspection of fixes		
EMSE	Namavar, Marjane, Noor Nashid, and Ali Mesbah	A controlled experiment of different code representations for learning-based program repair.	2022	https://link-springer-com.ezproxy1.lib.asu.edu/article/10.1007/s10664-022-10223-5	87	Namavar, Marjane, Noor Nashid, and Ali Mesbah. "A controlled experiment of different code representations for learning-based program repair." Empirical Software Engineering 27.7 (2022): 190.	Program repair; Deep learning; Code representation; Controlled experiment;	Program repair; Deep learning; Code representation; Controlled experiment; 	FALSE	TRUE	FALSE	TRUE	TRUE	Pemma	This paper focuses on which input code representation (which parts of code and in what format should be included in the vectorization of code, i.e., the input to ML techniques) learning-based. The authors solely focus on "name-related" bugs (e.g., swapped function parameters, wrong binary operators, et al.)	The authors deduplicated test and training datasets, performed after data extraction phase and before converting data into code representations. "In this study, we focus on learning from short buggy statements without context[.]"	FALSE	TRUE	TRUE	FALSE	FALSE	FALSE	FALSE	code corpus from open-source JavaScript projects from GitHub (150k JavaScript files with 68.6 MLOC)	Reptory	Yes	No	Yes	https://github.com/annon-reptory/reptory	Yes (per bug-type training, ranges from 4.75 hours to 41.32 hours)	Yes (CPU Intel Xeon 2.5GHz w/ 62GB mem)								FALSE	FALSE	TRUE	Tensorflow NMT (unidirection with LSTM as recurrent unit, Encoder-decoder)	TRUE	TRUE	scaled Luong attention	Not reported	Embedding size = 512, no context	FALSE			TRUE	Perfect Line; no context			FALSE	FALSE	"correct" not well defined, but based on context refers to consistent with developer-provided patch		
EMSE	Ginelli, Davide, et al	A comprehensive study of code-removal patches in automated program repair.	2022	https://link-springer-com.ezproxy1.lib.asu.edu/article/10.1007/s10664-021-10100-7	88	Ginelli, Davide, et al. "A comprehensive study of code-removal patches in automated program repair." Empirical Software Engineering 27.4 (2022): 97.	Automatic program repair; Code-removal patches; Software testing; Debugging;	Automatic program repair; Code-removal patches; Software testing; Debugging; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	TLDR: Patches consisting of removing code are insufficient to fix bugs  "Arja-Kali may not have a reliable behavior" "Benchmarks of [earlier studies by Long/Rinard, Qi et al., Martinez] used some kind of selection, which results in a biased sampling."	Uses previously published research results for jKali, Arja-Kali on Bears, Bugs.jar, Defects4J, IntroClassJava, QuixBugs datasets, to select between jKali and Arja-Kali	FALSE	FALSE	FALSE	FALSE	FALSE	TRUE	FALSE	Repairnator Travis CI projects						Yes (3 hour max-runtime per build, 100 minutes / jKali core repair loop)	Yes (CPU Intel Xeon E5-2690v4 (2x14 cores) 128GB mem / node)			jKali					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Repairnator-Experiments	FALSE	feature evaluator uses statement-level SBFL (GZoltar w/ Ochiai)			TRUE	TRUE	uses "test-suite adequate" vs "correct" patches "correct" if (1) the patch is identical or (2) semantically equivalent to the human-written patch		
EMSE	Luo, Linghui, et al	TaintBench: Automatic real-world malware benchmarking of Android taint analyses.	2022	https://link.springer.com/article/10.1007/s10664-021-10013-5	89	Luo, Linghui, et al. "TaintBench: Automatic real-world malware benchmarking of Android taint analyses." Empirical Software Engineering 27 (2022): 1-41.	Taint analysis; Benchmark; Real-world benchmark; Android malware;	Taint analysis; Benchmark; Real-world benchmark; Android malware; 	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper focuses on static taint analyses of Android applications	taint flow is the set of dataflow paths between associated taint source and sink NOTE FROM PEMMA: SKIPPING THIS AS IT IS NOT APR RELATED	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
EMSE	Yang, Deheng, et al	Where were the repair ingredients for Defects4j bugs? Exploring the impact of repair ingredient retrieval on the performance of 24 program repair systems.	2021	https://link.springer.com/article/10.1007/s10664-021-10003-7	90	Yang, Deheng, et al. "Where were the repair ingredients for Defects4j bugs? Exploring the impact of repair ingredient retrieval on the performance of 24 program repair systems." Empirical Software Engineering 26 (2021): 1-33.	Automated Program Repair; Fix Ingredient; Code Change Action; Donor Code;	Automated Program Repair; Fix Ingredient; Code Change Action; Donor Code; 	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper focuses on where "fix ingredients", "repair ingredients" occur or are available for bugs using Defects4J. "Donor code" is considered both the body of code from which these ingredients are sourced and the literal content. The authors find that more than half of bugs cannot be fixed (the relevant repair ingredients is not available in search space). THose bugs that are fixed by tools are addressed with basically a shallow set of changes.  They also indicate that mining code change histories from programs with large commit histories can benefit bug resolutions for programs with small commit histories. Additionally, partial fix ingredients can be found at disparate locations (non-contiguous donor code).	WHAT? "bug-triggering test cases are a rich source for donor code search." (does this mean that code statements/fix ingredients along execution path are more likely to be relevant/proper repair ingredients? -- authors are not clear in their explanation, but this seems to be right based on their figures.) Uses code differencing tools (GumTree) to determine set of change operations.  !!! This paper has a great table of Java-based APR systems that evaluated on Defects4J (tool name, authors, venue, year, correct vs plausible repair results !!!	FALSE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.4.0)								n/a							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J	FALSE	n/a	n/a	n/a	TRUE	TRUE	"correct" is a patch equivalent to the patch provided by developers in the benchmark (decided manually by authors)		
EMSE	Aleti, Aldeida, and Matias Martinez	E-APR: mapping the effectiveness of automated program repair techniques.	2021	https://link.springer.com/article/10.1007/s10664-021-09989-x	91	Aleti, Aldeida, and Matias Martinez. "E-APR: mapping the effectiveness of automated program repair techniques." Empirical Software Engineering 26 (2021): 1-30.	Automated program repair; Software features;	Automated program repair; Software features; 	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	"E-APR" => explaining APR, which identifies features of buggy programs and can be used to examine why APR tools succeed or fail to repair. The authors also ask whether or not existing APR datasets are significantly different - i.e., are APR tools generalizing to benchmarks' bugs or bugs in general?  The authors outline some features of existing datasets that have an impact on existing APR techniques. (DIVERSITY OF BENCHMARKS). Finally, the authors investivate ML techniques used in APR, particularly classification techniques. (revisit, this is phrased weirdly, could refer to their own classification mechanism?) Basically, the authors have extended earlier work on "instance space analysis" which objectively evaluates the performance of different algorithms (extended work applied to both ML and search-based software testing) to the APR domain. "Why can't a certain APR technique generate a plausible patch for a certain bugs"	this paper is a bit weird and a little all over the place.   but good quote:  "The overwhelming majority of published work in APR only describes the benefits of the newly introduced technique, while just a few mention the limitations or present negative results." This paper has a nice table of object-oriented features and their descriptions that can be used as code metrics  [PDR: ANY USABLE FOR ROBUSTNESS CODE METRICS?-- they use open-source tool "Coming" from https://github.com/SpoonLabs/coming ]. [PDR: ROBUSTNESS IDEA -- "Mutation applies random changes in code, and is less likely to introduce new bugs if classes are highly cohesive."  summary of results:  (1) mutation-based g-and-v apr tools struggle with programs with large proportion of user-defined types and private methods, but when methods are highly cohesive (the authors provide an alternate definition for this as "program elements are in the same place (in this case, class)", but this definition is weird - the table defines cohesion among methods as "the relatedness among methods of a class based upon the parameter list of the methods" soooo, maybe a value of 1 for (A,B) means that all parameters are the same for function A and function B? ...or that class variables and methods are used consistently?). (2) evaluated a number of ML classifiers to determine which can be used to select the most suitable APR technique using a buggy program's features.	FALSE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J, Bugs.jar, Bears, IntroClassJava, QuixBugs	E-APR			Yes, used experimental data from "RepairThemAll.experiment" 		not reported	not reported	n/a		RepairThemAll, jGenProg, GenProg-A, Cardumen, jKali, Kali-A, jMutRepair, Nopol, Dynamoth, RSRepair-A, ARJA, NPEFix					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J, Bugs.jar, Bears, IntroClassJava, QuixBugs	FALSE	feature evaluator uses statement-level SBFL (GZoltar w/ Ochiai) [top-100 statements is cut-off]	n/a	n/a	FALSE	FALSE	"correctness oracle" as incomplete proxy for correctness No full definition of "correct", but surmise from context that their use of "correct" means a general fix for the bug under scrutiny		
EMSE	Ye, He, Matias Martinez, and Martin Monperrus	Automated patch assessment for program repair at scale.	2021	https://link.springer.com/article/10.1007/s10664-020-09920-w	92	Ye, He, Matias Martinez, and Martin Monperrus. "Automated patch assessment for program repair at scale." Empirical Software Engineering 26 (2021): 1-38.	Automatic program repair; Automatic patch assessment;	Automatic program repair; Automatic patch assessment; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper focuses on automatic correctness assessments for evaluating patch candidates generated by APR systems.  Their method uses the human-written patch as ground-truth and generates random tests based on it, i.e., Random testing with Ground Truth. Automatic patch assessments using RGT improved the SOA by 190% and may be reliable enough to do overfitting analysis during their APR tool evaluations.	PDR IDEA: Benchmarks can add in hooks for "overfitting analyses"	FALSE	FALSE	FALSE	TRUE	TRUE	TRUE	TRUE	Defects4J	RGT			Yes, but at an "anonymous.4open.science" link??				not reported		DiffTGen					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J	FALSE		n/a	available as dataset	FALSE	FALSE	"correctness" - correct patches are good general solutions to the bug under consideration. 		
EMSE	Koyuncu, Anil, et al	Fixminer: Mining relevant fix patterns for automated program repair.	2020	https://link.springer.com/article/10.1007/s10664-019-09780-z	93	Koyuncu, Anil, et al. "Fixminer: Mining relevant fix patterns for automated program repair." Empirical Software Engineering 25 (2020): 1980-2024.	Fix patterns; Patches; Program repair; Debugging; Empirical software engineering;	Fix patterns; Patches; Program repair; Debugging; Empirical software engineering; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper presents a systematic, automated approach to obtaining relevant and usable fix patterns that can be used in other patch generation systems. Their approach obtain these code change patterns from AST differencing tools, available as "rich edit scripts".  These "rich edit scripts" is a representation that describes the code change in terms of context, operations performed, and tokens involved.	FixMiner is not an APR tool, but augments APR tools that employ pattern-based fixes.	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0)	FixMiner	yes ( https://github.com/SerVal-DTF/fixminer-core ) <= turns out, not a valid link anymore -- maybe https://github.com/SoundaryaTekkalakota/fixminer-core				Yes (RETrees construction: 17 minutes; comparing RETrees: 18minutes)	Yes (computing system w/ 24 Intel Xeon E5-2680 v3 cores @ 2GHz with 3TB RAM)	not reported		PAR					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J	FALSE	augmented APR tool uses statement-level SBFL (GZoltar w/ Ochiai)	n/a	not reported	TRUE	TRUE	"plausible" vs "correct" terminology used --  "Correct" -- manually check equivalence between "plausible" patch and oracle patch provided, semantically similar patches are considered correct		
EMSE	Kim, Jindae, and Sunghun Kim	Automatic patch generation with context-based change application.	2019	https://link.springer.com/article/10.1007/s10664-019-09742-5	94	Kim, Jindae, and Sunghun Kim. "Automatic patch generation with context-based change application." Empirical Software Engineering 24.6 (2019): 4071-4106.	Automatic program repair; Automatic patch generation; Context-based change application;	Automatic program repair; Automatic patch generation; Context-based change application; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper introduces a patch generation method that uses contexts from AST-changes of human-written patches to determine applicable patches. 	This method basically searches a repository of abstracted AST changes to identify potentially applicable changes based on the context of AST changes. 	TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (partial-357 bugs)	ConFix	yes ( https://github.com/thwak/ConFix )				Yes (2 hour max-runtime per ConFix session, may end early if generated the max candidate number)	Yes (Amazon EC2 instance (m5.xlarge) with 4 CPUs and 16GB memory)	not reported							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J	FALSE	Line SBFL (Ochiai)	"To provide changes for patch generation, we collected over 6K human-written patches from non different projects and identified about 216K abstract AST changes (44K unique changes). We also identified 38K (more specific) and 2.4K (more abstract) contexts for the collected changes."	per-bug available at https://github.com/thwak/confix2019result	TRUE	FALSE	"manually inspected generated patches" to assess correctness.  uses "precision" (correct / plausible) and "recall" (plausible)  Authors inspected developer fixes to determine if a patch existed in their tool's search space, ONLY THEN it would be reported in the total.		
EMSE	Yu, Zhongxing, et al	Alleviating patch overfitting with automatic test generation: a study of feasibility and effectiveness for the Nopol repair system.	2019	https://link.springer.com/article/10.1007/s10664-018-9619-4	95	Yu, Zhongxing, et al. "Alleviating patch overfitting with automatic test generation: a study of feasibility and effectiveness for the Nopol repair system." Empirical Software Engineering 24 (2019): 33-67.	Program repair; Synthesis-based repair; Patch overfitting; Automatic test case generation;	Program repair; Synthesis-based repair; Patch overfitting; Automatic test case generation;	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	Test suite are incomplete specifications of a program and therefore can lead to overfitting repairs, i.e., repairs that fail to generalize to the actual bug. This paper attempts to mitigate overfitting by generating additional test cases (automatic test case generation) so that synthesis-based tools' internal representation of repair constraints are strengthened.	UnsatGuided augments synthesis-based repair tools.  Tool pipeline used in evaluation is: Nopol (synthesis technique); EvoSuite (automatic test case generation)	TRUE	FALSE	FALSE	TRUE	TRUE	FALSE	TRUE	Defects4J (partial-224 bugs)	UnsatGuided	Not reported				Yes (40 minutes [Nopol global timeout value])	Yes (Cluster consisting of 200 virtual nodes running Ubuntu 16.04 on a single Intel 2.68 GHz Xeon core with 1GB of RAM.)	n/a		Nopol					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	n/a	n/a	n/a	FALSE	FALSE	"correct" maintains expected behavior of underlying program, regardless of completeness of test suite, while addressing underlying bug		
EMSE	Motwani, Manish, et al	Do automated program repair techniques repair hard and important bugs?.	2018	https://link.springer.com/article/10.1007/s10664-017-9550-0	96	Motwani, Manish, et al. "Do automated program repair techniques repair hard and important bugs?." Proceedings of the 40th International Conference on Software Engineering. 2018.	Automated program repair; Repairability;	Automated program repair; Repairability;	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	What kinds of bugs do APR tools repair well or poorly?  Authors say that "We use results from prior evaluations of these techniques on the ManyBugs and Defects4J datasets."  Models whose prior evaluations were used: GenProg, TrpAutoRepair, AE, Kali, SPR, Prophet (C); Nopol, jGenProg, jKali (Java)	the authors use "repairability" differently from the "A correlation study between apr and test-suite metrics" paper, opting for diversity of bugs that are capable of being fixed by an APR tool, instead of "are generated APR repairs correct?" oof, this paper found that neither the manual repair time for a defect nor the test suite's coverage correlate with a defect's patchability by APR. (q: are they using this "time to repair" as a proxy for complexity? Looks like "yes, time to repair is used as a proxy for complexity and priority") q: do ML methods address the gaps left by traditional APR techniques, specifically bugs that require adding loops, new function calls, or updating method signatures?  (does this paper indicate how often each of these gaps occurs in their benchmark?)	FALSE	FALSE	FALSE	FALSE	TRUE	TRUE	TRUE	ManyBugs, Defects4J		Not reported				Not reported	Not reported	n/a							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	n/a	n/a	n/a	FALSE	FALSE	"developer-judged" correctness		
EMSE	Yi, Jooyong, et al	A correlation study between automated program repair and test-suite metrics.	2018	https://link.springer.com/article/10.1007/s10664-017-9552-y	97	Yi, Jooyong, et al. "A correlation study between automated program repair and test-suite metrics." Proceedings of the 40th International Conference on Software Engineering. 2018.	Automated program repair; Test suite; Empirical evaluation; Correlation;	Automated program repair; Test suite; Empirical evaluation; Correlation;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper proposes and attempts to answer the question: can traditional test-suites be used for APR purposes?  They also include a correlation study between assorted test-suite metrics and "the reliability of generated repairs". (Reliability == "whether regressions occur in a repair" => does the repair pass a heldout test suite?). "[I]ncreasing statement coverage is likely to be more effective [at reducing regression ratio] than improving the other test-suite metrics such as branch coverage."		FALSE	FALSE	FALSE	TRUE	TRUE	FALSE	FALSE	4 large-scale real-world programs: {php, libtiff, ...} + SIR benchmark		Not reported				Not reported	Not reported	n/a		GenProg, SemFix					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	n/a	n/a	n/a	FALSE	FALSE	"correct" patch -- a patch that not only passes all tests available to a repair system, but also indeed fixes the bug.		
EMSE	Oliveira, Vinicius Paulo L	Improved representation and genetic operators for linear genetic programming for automated program repair.	2018	https://link.springer.com/article/10.1007/s10664-017-9562-9	98	Oliveira, Vinicius Paulo L., et al. "Improved representation and genetic operators for linear genetic programming for automated program repair." Empirical Software Engineering 23 (2018): 2980-3006.	Automatic software repair; Automated program repair; Genetic improvement; Genetic programming; Crossover operator; Mutation operator;	Automatic software repair; Automated program repair; Genetic improvement; Genetic programming; Crossover operator; Mutation operator;	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	This paper outlines a "reformulation of program repair representation, crossover, and mutation operators" to better traverse the operator, fault, and fix subspaces of search-base heuristics. It extends the existing GenProg algorithm with these new components.		TRUE	FALSE	FALSE	FALSE	TRUE	FALSE	FALSE	IntroClass, {gcd, zune}, ManyBugs {libtiff}	"GenProg with Subspace Mutation"	Not reported				Not reported	Not reported	n/a		GenProg					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	reuses GenProg infrastructure	n/a	n/a	FALSE	FALSE	sidesteps "plausible" vs "correct" argument by "evaluating in part on well-specified programs"		
EMSE	Le, Xuan-Bach D	Overfitting in semantics-based automated program repair.	2018	https://link.springer.com/article/10.1007/s10664-017-9577-2	99	Le, Xuan-Bach D., et al. "Overfitting in semantics-based automated program repair." Proceedings of the 40th International Conference on Software Engineering. 2018.	Automated program repair; Program synthesis; Symbolic execution; Patch overfitting;	Automated program repair; Program synthesis; Symbolic execution; Patch overfitting;	FALSE	TRUE	FALSE	TRUE	TRUE	Pemma	outlines overfitting in semantics-based APR  This paper evaluates overfitting in the context of Angelix, an APR tool the employs semantics-based search. Angelix uses expression-level SBFL (ochiai) for fault localization, while symbolic execution is used to infer a program's specification as a set of "correctness constraints".  It finds that Angelix is subject to similar overfitting as heuristics-based APR tools. 		FALSE	FALSE	FALSE	TRUE	TRUE	FALSE	FALSE	IntroClass, Codeflaws		n/a						n/a		Angelix					FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	n/a	n/a	n/a	FALSE	FALSE	"correct" - general fix for the bug		
EMSE	Yu, Tingting, and Michael Pradel	Pinpointing and repairing performance bottlenecks in concurrent programs.	2018	https://link.springer.com/article/10.1007/s10664-017-9578-1	100	Yu, Tingting, and Michael Pradel. "Pinpointing and repairing performance bottlenecks in concurrent programs." Empirical Software Engineering 23 (2018): 3034-3071.	Testing; Concurrency; Performance bottlenecks;	Testing; Concurrency; Performance bottlenecks;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	SyncProf is a profiling approach that uses dynamic analyses to identify, localize, and fix (propose fixes for) performance issues/bottlenecks in concurrent programs.	Implementation generates synchronization behavior in a "Synchronization Dependence Graph", which outlines edges of causality between dynamic instances of some concurrent programs critical sections Required inputs: program + test cases	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	Not a singular benchmark, but several benchmark programs used by others were used in this tool's evaluation	SyncProf	Not reported	n/a	Not reported		Yes (Analysis time reported per case)	Not reported	n/a							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE	n/a	n/a	n/a	FALSE	FALSE			
ICSE	Xia, Chunqiu Steven, Yuxiang Wei, and Lingming Zhang	Automated program repair in the era of large pre-trained language models.	2023	http://lingming.cs.illinois.edu/publications/icse2023a.pdf	101	Xia, Chunqiu Steven, Yuxiang Wei, and Lingming Zhang. "Automated program repair in the era of large pre-trained language models." Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association for Computing Machinery. 2023.	Automated Program Repair; Machine Learning;	Codes; Computer bugs; Maintenance engineering; Software; Distance measurement; Task analysis; Faces;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper focuses on LLM-based apr methods for (1) function replacement, (2) code "chunk" replacement, (3) single line fixes.	"larger models tend to deliver stronger APR results" "Since we only have access to the training data used in CodeT5, GPT-Neo, GPT-J and GPT-NeoX models, we further check if the fixed function is within the training datasets when the correct patch is equivalent to the developer fix for these models. We found that while 38% (48/128) of bugs fixes contain only the same fix as the developer patch, only 15% (20/128) of those patches are also found in the original training data, showing that the majority of correct bug fixes provided by these LLMs are not simply from memorizing the training data."	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0)		LLM models leveraged: GPT-NEO, GPT-J, GPT-NEOX, Codex, CodeT5, InCoder	not reported	no finetuning	not provided	Not reported	Yes (32-core workstation with Ryzen Threadripper PRO 3975WX CPU, 256GB RAM and NVIDIA RTX A6000 GPU)	individual times not reported, but "generating 200 patches for each of the 3 settings costs no more than 2.5 hours for each model"			Since we only have access to the training data used in CodeT5, GPT-Neo, GPT-J and GPT-NeoX models, we further check if the fixed function is within the training datasets when the correct patch is equivalent to the developer fix for these models. We found that while 38% (48/128) of bugs fixes contain only the same fix as the developer patch, only 15% (20/128) of those patches are also found in the original training data, showing that the majority of correct bug fixes provided by these LLMs are not simply from memorizing the training data.   Moreover, our RQ4 shows that improvements can be further made by combining repair templates with LLMs, which is orthogonal to the data leakage issue. Additionally, We observe that LLMs are able to achieve the state-of-the-art results on QuixBugs dataset which is not part of the training data as it has low number of stars on GitHub and contains synthetic bugs and patches that are not part of any larger real-world projects. Further reducing the data leakage issue would require retraining the LLMs, which could be extremely costly.	 We evaluate on all bugs in Defects4J 1.2 by adjusting our infilling-style repair settings, following AlphaRepair		• GPT-Neo [42], GPT-J [49], GPT-NeoX [50] All three models are open-source implementations of the GPT-3 transformer architecture [27]. In our experiments, we use GPT-Neo models with 125M, 1.3B and 2.7B parameters. GPT-J and GPT-NeoX are even larger models with 6.7B and 20B parameters. These models were trained on The Pile [51], an 800GB dataset combining 22 diverse text-based datasets with 7.6% containing open-source Github code. • Codex [28] A 12B parameter GPT-3 based model designed for code generation. Codex is initialized with GPT-3 weights trained on natural language corpus and then finetuned on a large corpus of 159GB code files. 2) Infilling Models: • CodeT5 [52] A 220M parameter model based on T5 [43] architecture designed for code related tasks. CodeT5 is trained using span prediction objective on 8.35 million functions across 8 different programming languages by combining CodeSearchNet [53] with C/C# dataset from BigQuery [54]. • INCODER [33] A model designed for code infilling by adopting a causal masking objective [45]. INCODER is trained on both open-source Github/GitLab code (159 GB) and StackOverFlow questions and answers (57 GB). We use both the 1.3B and 6.7B parameter version. • Codex In addition to using Codex as a generative model, we use the recently added suffix feature [55] to perform code infilling. Since Codex is not open-sourced, we do not know how the model performs the infilling.	TRUE	TRUE	TRUE	LLM	TRUE	TRUE	not reported		(1) function, (2) code "chunk", (3) line	FALSE		Defects4J (v1.2.0)	TRUE	function, chunk, line		summary	TRUE	TRUE	" To determine correct patches, we follow the standard practice in APR research and manually inspect each plausible patch for semantic equivalency."		
ICSE	Fan, Zhiyu, et al	Automated repair of programs from large language models.	2023	https://ieeexplore.ieee.org/iel7/10172484/10172342/10172854.pdf?casa_token=WsKY9wJ5DFwAAAAA:gkSqO7DW7fe0CWBEUQM1OfR0qhe9OCtJXQIHO22OUtioXMnFf9ZgeCZRSOb3b2SpDPOSFRVbfQ	102	Fan, Zhiyu, et al. "Automated repair of programs from large language models." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.	Large Language Model; Program Repair;	Location awareness; Training; Analytical models; Codes; Semantics; Maintenance engineering; Programming;	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	Can APR techniques enhance LLM-generated code for LeetCode contests?		FALSE	TRUE	TRUE	FALSE	TRUE	FALSE	FALSE	LMDefects (Codex-generated LeetCode solutions)					https://github.com/zhiyufan/apr4codex	Not reported	Yes (All experiments were conducted on an Ubuntu- 16.04 server, with 64GB RAM and Intel(R) Xeon(R) CPU E5-2660 @ 2.00GHz, and NVIDIA Titan V GPU)	not reported		Codex					FALSE	TRUE	TRUE	Codex code-davinci-002 and Codex-e code-davinci-edit-001	TRUE	TRUE	n/a	n/a	n/a	TRUE	113 Java programming tasks (46 solved, 67 unsolved)	LMDefects (Codex-generated LeetCode solutions)	TRUE	SBFL, line, Ochiai	n/a	summary by defect and sub category	TRUE	TRUE	"Correct patches are patches that make the incorrect solutions pass both the public tests and private tests and accepted by LeetCode."		
ICSE	Meng, Xiangxin, et al	Template-based Neural Program Repair.	2023	https://ieeexplore.ieee.org/iel7/10172484/10172342/10172686.pdf?casa_token=3foIHeqMkgwAAAAA:1ZnAXlwLyzYrAlPYFl4N0ZEDWrGZJ92qvREF3yZCPsy8sQcRRVgoFWlTeZABZ8uRFSReBWaA0g	103	Meng, Xiangxin, et al. "Template-based Neural Program Repair." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.	automated program repair; fix templates; neural machine translation; deep learning;	Location awareness; Codes; Source coding; Computer bugs; Semantics; Maintenance engineering; Benchmark testing;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	this paper introduces an APR tool that combines template-based APR methods with neutral machine translation models.		TRUE	TRUE	TRUE	FALSE	TRUE	TRUE	TRUE	Defects4J (v1.2.0, v2.0.0)	TENURE	in replication pkg		in replication pkg	https://github.com/mxx1219/TENURE	Running (we set the running time as 5 hours for each bug version, )	Yes ( 20 cores of 2.4GHz CPU, 384GB RAM and NVIDIA Tesla V100 GPUs with 32 GB memory)	5hr upper limit run-time per bug							FALSE	FALSE	TRUE	LSTM-based encoder/decoder	TRUE	TRUE	global attention 	not reported		FALSE			TRUE	perfect and Ochiai SBFL, line		summary	TRUE	TRUE	"all plausible patches are double-checked by two authors, and a patch is only be considered correct if both authors acknowledge its correctness."		
ICSE	Zhu, Qihao, et al	Tare: Type-Aware Neural Program Repair.	2023	https://ieeexplore.ieee.org/iel7/10172484/10172342/10172781.pdf?casa_token=eCY36ZLQBFMAAAAA:HirQTFwAHBHg3adEVDuW5s_fFZmYjeKGLkJKsOdECvm6p2UH8ihMCWi_LUsYlcP_LI-Q37Ridw	104	Zhu, Qihao, et al. "Tare: Type-Aware Neural Program Repair." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.	program repair; neural networks;	Deep learning; Codes; Computer bugs; Maintenance engineering; Benchmark testing; Software; Generators;	FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	This paper incorporates typing into source code grammar and AST, that's used in their neural program repair tool Tare. 		TRUE	TRUE	TRUE	FALSE	TRUE	TRUE	TRUE	Defects4J (v1.2.0, v2.0.0), QuixBugs	Tare	only analysis code disclosed		in replication pkg	https://doi.org/10.5281/zenodo.7029404	Training (48 mins for an epoch on two Nvidia Titan 3090 with batch size 60 for Tare, whereas 53 mins for Recoder. These two models both are trained for 20 epochs.) running (90s avg / fault location with beamsize 100)	Yes (Nvidia Titan 3090)	90s per fault locale w/ beamsize 100		Recoder					FALSE	FALSE	TRUE	Recoder	TRUE	TRUE	attention layer adapted to encode heterogeneous graphs	not reported		FALSE		Defects4J (v1.2.0, v2.0.0), QuixBugs	TRUE	perfect and Ochiai SBFL, line		per-bug is available	TRUE	TRUE	"The plausible patch is considered as correct when it is identical or semantically equivalent to the developer-written patch judged by two authors individually. To alleviate the potential error in this processing, we also publish all generated patches for public judgment (details are in Section XI)."		
ICSE	Jiang, Nan, et al	Impact of code language models on automated program repair.	2023	https://ieeexplore.ieee.org/document/10172517	105	Jiang, Nan, et al. "Impact of code language models on automated program repair." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.	Automated Program Repair; Code Language Model; Fine-Tuning; Deep Learning;	Codes; Computer bugs; Memory management; Training data; Benchmark testing; Maintenance engineering; Software;	FALSE	TRUE	TRUE	FALSE	TRUE	Pemma	This work evaluates 10 code language models on 4 APR benchmarks and compares against DL-based APR tools. In particular, HumanEval-Java is a new APR benchmark that no existing CLMs ahve seen during pre-training (focus on fairness of evaluations)  pseudo-ablation study	• Finding 1: CLMs, even without fine-tuning, have com- petitive fixing capabilities. To our surprise, the best CLM, as is, fixes 72% more bugs than the state-of-the-art DL- based APR techniques. • Finding 2: While buggy lines (code lines that need to be modified) are useful to guide fixing, CLMs fail to make good use of them and fix fewer bugs when the buggy lines are explicitly given.	FALSE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0, v2.0.0), QuixBugs, HumanEval-Java		in replication pkg	in replication pkg	in replication pkg	https://github.com/lin-tan/clm	Running reported	Yes 	time efficiency is studied per model		PLBART, CodeGen, CodeT5, InCoder, KNOD, CURE, Reward, Recoder, 					FALSE	FALSE	TRUE	10 models that use 4 different ML architectures	TRUE	TRUE	see each model	not reported	not reported	TRUE	small programs, "realistic code refinement and not included in CLMs pretraining data", converted from Python to Java	Defects4J (v1.2.0, v2.0.0), QuixBugs, HumanEval-Java	FALSE	not specified		per bug available in replication package	TRUE	TRUE	"And we finally manually check the correctness of plausible patches to distinguish correct patches (which should be identical or semantically equivalent to developer-written patches)."		
ICSE	Li, Zongjie, et al	Cctest: Testing and repairing code completion systems.	2023	https://ieeexplore.ieee.org/iel7/10172484/10172342/10172845.pdf?casa_token=gyY4txJin5wAAAAA:l-ckwc9kK0qdBy3--Ovbh-TlPpo5z1VyfTrah3372xB05f-EgudK1xluJi4taectIud42yOYHQ	106	Li, Zongjie, et al. "Cctest: Testing and repairing code completion systems." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.		Deep learning; Codes; Source coding; Closed box; Maintenance engineering; Task analysis; Programming profession;	TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	This work focuses on code completion systems like GitHub Copilot, that function like an AI pair programmer. However, the results of these AI systems are often confusing and sometimes erroneous, sub-optimal.  Subtle changes in the input context can render quite different results (i.e., code snippets).	skipping...	FALSE	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE			FALSE				FALSE	FALSE			
ICSE	Motwani, Manish, and Yuriy Brun	Better automatic program repair by using bug reports and tests together.	2023	https://ieeexplore.ieee.org/iel7/10172484/10172342/10172693.pdf?casa_token=v7m4UHIWqHQAAAAA:mO16wUoJ8XxvbjotdWLutyiBcDOAFWcdW-PoYZOaLt1yL1N0zJ3e_TovUX9mTr6MlcEgqYhzSQQ	107	Motwani, Manish, and Yuriy Brun. "Better automatic program repair by using bug reports and tests together." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.	Automatic Program repair; Information retrieval based fault localization; Debugging; fault localization; 	Location awareness; Industries; Computer bugs; Training data; Maintenance engineering; Benchmark testing; Software; 	FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	The authors outline a new mechanism to improve fault localization using SBFL and IR (information retrieval) (current FL methods are imperfect and are often the main reason for incorrect patches with APR). The authors update existing APR tools (Arja, SequenceR, Simfix) to use this new FL mechanis and evaluate the quality of the patches produced. 		FALSE	TRUE	FALSE	FALSE	TRUE	FALSE	TRUE	Defects4J (v2.0.0)	SBIR, Blues	in replication pkg	n/a	in replication pkg	https://doi.org/10.7910/DVN/OHMYAK	Running (Executing our study is highly computationally intensive and required eight weeks of wall-clock time on a 50-node cluster. )	Yes (a cluster of 50 compute nodes, each with a Xeon E5-2680 v4 CPU with 28 cores (2 processors, 14 cores each) running at 2.40GHz. Each node had 128GB of RAM and 200GB of local SSD. )	held-out test suite generation: EvoSuite time budget - (12min/seed)							FALSE	FALSE	FALSE	n/a	FALSE	FALSE	n/a	n/a	n/a	FALSE		Defects4J (v2.0.0)	FALSE	SBIR, Blues		per-bug available in replication package	TRUE	FALSE	"If all the evaluation tests pass, we manually compare the patch against the developer’s patch. If the patch is semantically equivalent to the developer’s patch, we annotate it as correct. If it is not, we annotate it as plausible."		
ICSE	Jiang, Nan, et al	Knod: Domain knowledge distilled tree decoder for automated program repair.	2023	https://ieeexplore.ieee.org/document/10172873	108	Jiang, Nan, et al. "Knod: Domain knowledge distilled tree decoder for automated program repair." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.	Automated Program Repair; Abstract Syntax Tree; Deep Learning;		FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	The authors proposed a DL-based approach to APR that focuses on improving the quality of generated patch candidates. Note, DL approaches often generate syntactically and semantically incorrect patch candidates.	"While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug."	TRUE	TRUE	TRUE	FALSE	TRUE	FALSE	TRUE	Defects4J (v1.2.0), QuixBugs, Defects4J (v2.0.0)	Knod	in replication pkg	not available, but training data + replication package and KNOD model is available in code	Our training data is shared atvia Zenodo at https://doi.org/10.5281/zenodo.7570475.	https://github.com/lin-tan/knod and Our training data is shared atvia Zenodo at https://doi.org/10.5281/zenodo.7570475.	"During validation, we set a five-hour running-time limit, "; TRAINING TIME NOT REPORTED	"We train KNOD on one 56-core server with eight NVIDIA GeForce RTX 2080 TI GPUs, and evaluate KNOD on the same server with one NVIDIA GeForce RTX 2080 TI GPU."								FALSE	FALSE	TRUE	3 stage, tree-decoder	TRUE	TRUE	ASG self-attention and sequence self-attention; fairly well-dectailed, with ASG self-attention dependencies between adjacent nodes on ASG node sequence leveraged; "Different from traditional tree self-attention, which consideres node features only, our ASG self-attention considers both node and edge features."	not reported	"buggy project" => buggy functions, location of buggy lines, ASG node sequences and adjacency matrix	FALSE			TRUE	Perfect function		summary in paper, plausible and correct repairs available in their replication package	TRUE	TRUE	"we manually check the correctness of plausible patches returned by KNOD. We consider a plausible patch correct if it is semantically equivalent to developer patches. The labeling procedure involves two participants. The agreement ratio is 92.1% and inconsistent cases are resolved by further discussion." BEAM SEARCH = 1000 ! ! ! ! ! !	5hr time limit	beam size = 1,000
ICSE	Parasaram, Nikhil, Earl T	Rete: Learning Namespace Representation for Program Repair.	2023	https://ieeexplore.ieee.org/iel7/10172484/10172342/10172760.pdf?casa_token=grrb3-JjF_AAAAAA:w6kSYaWUd6buB9Q6SxXYZW6JU5c00WUWboKfyD2XA2uwLmZgGQISyzG6a-Vd-BKRBkuhz_1BDQ	109	Parasaram, Nikhil, Earl T. Barr, and Sergey Mechtaev. "Rete: Learning Namespace Representation for Program Repair." 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023.	Program Repair; Deep Learning; Patch Prioritisation; Variable Representation;		FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	Rete is a loosely architecture set of components that can be used to  existing APR tools to improve their performance and patch generation efficiency and prioritization. They've applied Rete to both Python and C, porting existing APR tool Prophet to Python, which it then augments with Rete. Both Python-Prophet flavors are compared to CoCoNuT's python results and improve upon them. 		TRUE	TRUE	TRUE	TRUE	TRUE	FALSE	FALSE	BugsInPy, ManyBugs	Rete	in replication pkg	not available, but training data + replication package and KNOD model is available in code	evaluation datasets in replication package, 90-10 split for training-testing	RETE’s implementation, and the scripts and data used to evaluate it, can be found in the accompanying package https: //github.com/norhh/Rete	4hr timeout (MAX RUN TIME)	We conducted experiments inside a Docker container on a CPU of 2.7 GHz machine running on Ubuntu 21.04 with 16GB of memory and Geforce RTX 3070M. 	not reported		Prophet; Trident		Our corpus consists of three bug datasets whose bugs belong to RETE’s defect class (Section III-A), as shown in Table II. The programs in P28 were sampled uniformly from GitHub on 21/10/21. We exclude two samples namely wireshark-37122-37123 and gzip-3fe0caeada-39a362ae9d from MB37 used in Trident [9], since we could build them along with our ML libraries. MB35 and BG107 include a test suite; the number of tests averages 62.8 for BG107 and 91.3 for MB35. P28 does not need one as we used it to train and evaluate RETE’s neural variable ranker.  We split the test suite into two parts (20-80), taking care that the smaller part includes at least one failing test. The smaller part is sent to a subject APR tool for patch generation. A patch is plausible if it passes the test suite. We consider a patch correct if it is plausible, and manual inspection deems it equivalent to the patch written by the developer. All the patches generated and used in this evaluation are available in the reproduction package. For all training and fine-tuning, we used the buggy datasets with a split of 90-10 for training and testing. All the hyperparameters are tuned by using K-fold cross-validation with k = 5 and grid search. No layers were frozen, since we had a large enough dataset and a set of low learning rates during the grid search. We use Adam optimiser with a weight decay fix [37].	To fine-tune the model, we trained the task-specific submodel on interleaved CDU chains so that the training data matches the prediction queries. To produce the shared hole needed for interleaving, we converted each input program into a set of single-holed programs, each with a different variable replaced with a hole. We interleaved the set of CDU chains that pass through each single-holed program’s holed statement.	CodeBERT --	TRUE	TRUE	TRUE	Uses CodeBERT in its variable prioritizer	TRUE	TRUE	nope, just vaswani ref	codebert ~Billions?	use compressed CDU chains with duplicates removed, reduced to two chains that use the most variables, truncated to 512 (CodeBERT window size)	FALSE		BugsInPy (107 bugs) ManyBugs-Trident subset (??)	TRUE	perfect line	buggy program, suspicious line number	summary in paper, plausible and correct repairs available in their replication package	TRUE	TRUE	A patch is plausible if it passes the test suite. We consider a patch correct if it is plausible, and manual inspection deems it equivalent to the patch written by the developer. All the patches generated and used in this evaluation are available in the reproduction package.		
TSE	Banerjee, Chong, Ballabriga, and Roychoudhury	EnergyPatch: Repairing Resource Leaks to Improve Energy-Efficiency of Android Apps	2018	https://ieeexplore.ieee.org/document/7889026	110	A. Banerjee, L. K. Chong, C. Ballabriga and A. Roychoudhury, "EnergyPatch: Repairing Resource Leaks to Improve Energy-Efficiency of Android Apps," in IEEE Transactions on Software Engineering, vol. 44, no. 5, pp. 470-490, 1 May 2018, doi: 10.1109/TSE.2017.2689012.	Mobile apps; energy bugs; non-functional testing; energy-aware test generation;		FALSE	FALSE	FALSE	FALSE	TRUE	Joe	This is a program optimization paper put in APR terms (largely because of the senior author, I suspect). Good work, but not APR unless you consider "could be optimized" as equivalent to "has a bug."		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Barbosa, Eiji Adachi, and Alessandro Garcia	Global-aware recommendations for repairing violations in exception handling.	2018	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8453159	111	Barbosa, Eiji Adachi, and Alessandro Garcia. "Global-aware recommendations for repairing violations in exception handling." Proceedings of the 40th International Conference on Software Engineering. 2018.	Exception Handling; Recommender system; Software maintenance;		TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	extended abstract only (journalist paper)		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Habayeb, Mayy, et al	On the use of hidden markov model to predict the time to fix bugs.	2017	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8052546	112	Habayeb, Mayy, et al. "On the use of hidden markov model to predict the time to fix bugs." IEEE Transactions on Software Engineering 44.12 (2017): 1224-1244.	Bug repositories; temporal activities; time to fix a bug; hidden markov model;		TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	This is not an APR paper, but tries to predict the amount of time that manual repairs may take for a bug.		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Di Nucci, Dario, et al	A developer centered bug prediction model.	2017	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/7835258	113	Di Nucci, Dario, et al. "A developer centered bug prediction model." IEEE Transactions on Software Engineering 44.1 (2017): 5-24.	Scattering metrics; bug prediction; empirical study; mining software repositories;		TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	Not related to APR, but bug occurrence patterns and prediction		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Wang, Zhe, et al	Using local clocks to reproduce concurrency bugs.	2017	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/document/8038023	114	Wang, Zhe, et al. "Using local clocks to reproduce concurrency bugs." IEEE Transactions on Software Engineering 44.11 (2017): 1112-1128.	Concurrency; bug reproducing; local clock;		TRUE	FALSE	FALSE	FALSE	TRUE	Pemma	Not related to APR, but focuses on techniques that ensure concurrency bugs are reproducible		FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			
TSE	Santiesteban, Priscila, et al	CirFix: Automated Hardware Repair and Its Real-World Applications.	2023	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/stamp/stamp.jsp?tp=&arnumber=10108500	115	Santiesteban, Priscila, et al. "CirFix: Automated Hardware Repair and Its Real-World Applications." IEEE Transactions on Software Engineering (2023).	Circuit designs; automated repair; empirical study; user study;		FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	automated repair for verilog		TRUE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	custom dataset	CirFix	yes	n/a	yes	https://github.com/hammad-a/verilog_repair	Each individual trial was terminated after 8 generations of evolution or 12 hours of wall-clock time (whichever came first).	conducted on a quad-core 3.4 GHz machine with hyperthreading and 16 GB of memory	time taken per buggy input is presented							FALSE	FALSE	FALSE		FALSE	FALSE				TRUE	verilog benchmark suite of 32 defect scenarios [22] based on three hardware experts — two from industry and one from academia — asked to transplant bugs they observed in real life into 11 different Verilog projects. 		FALSE	proprietary FL for logic 		per-bug	TRUE	TRUE	CirFix then performs a bit-level comparison of output wires against information for expected behavior to assess functional correctness of candidate repairs CirFix can produce plausible repairs for 21 out of the 32 Verilog defect scenarios within reasonable resource bounds, of which 16 are deemed correct upon manual inspection.		
TSE	Li, Leping, et al	Generating Concise Patches for Newly Released Programming Assignments.	2022	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/stamp/stamp.jsp?tp=&arnumber=9720157	116	Li, Leping, et al. "Generating Concise Patches for Newly Released Programming Assignments." IEEE Transactions on Software Engineering 49.1 (2022): 450-467.	Feedback generation; program repair; programming assignments;		FALSE	FALSE	FALSE	FALSE	TRUE	Pemma	this paper focuses on using apr as feedback to novice programmers in the context of MOOC programming coursework.	"However, experimental results [4] suggest that general APR techniques do not perform well on this task because bugs made by students are often different from those made by professional developers, and they cannot take advantage of the reference programs or multiple submissions that are available in the scenario of this task."	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	FALSE	Codeforces (500 programs, 50 programs for 10 program assignments)	AssignmentMender	yes	n/a	yes	https://github.com/CoPaGe/FeedbackExperimentTask	We set the time limit for the repair of each bug to five minutes	Our experiment is conducted on a 64-bit Windows server with one Intel(R) Core CPU and 16 GB RAM. 	not reported		Clara; Refactory					FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE	statement-based SBFL, Jaccard metric		summary, but per-bug experiments are duplicatable potentially	FALSE	TRUE	The fifth limitation is that AssignmentMender judges the correctness of the repaired program based on test cases. Thus, it may generate plausible patches that are not correct but can pass all test cases because of the risk that some plausi- ble reference programs might mislead AssignmentMender. In our experiment, all of the generated patches are correct. One possible reason is that those student programming assign- ments are much simpler than real word programs, and the associated test cases published on Codeforces are enough to characterize the specification of expected functions		
TSE	Le-Cong, Thanh, et al	Invalidator: Automated patch correctness assessment via semantic and syntactic reasoning.	2023	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/stamp/stamp.jsp?tp=&arnumber=10066209	117	Le-Cong, Thanh, et al. "Invalidator: Automated patch correctness assessment via semantic and syntactic reasoning." IEEE Transactions on Software Engineering (2023).	automated patch correctness assessment; automated program repair; code representations; overfitting problem; program invariants;		FALSE	TRUE	FALSE	FALSE	TRUE	Pemma	CodeBERT is used to extract syntactic features from code; INVALIDATOR uses a learning-based model that leverages syntactic reasoning to estimate the probability that the APR-patched program is over- fitting. 	"In this paper, we propose a novel technique, INVALIDATOR, to automatically assess the correctness of APR-generated patches via semantic and syntactic reasoning. INVALIDATOR leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model." 	FALSE	FALSE	TRUE	TRUE	TRUE	FALSE	TRUE	Defects4J	INVALIDATOR	https://github.com/ thanhlecongg/Invalidator		https://doi.org/10.5281/zenodo.7699142	All materials including implementa- tion, datasets, and experimental results are also published via https://doi.org/10.5281/zenodo.7699142	With respect to time efficiency, we limit 5 hours for invariant inference for each patch in our dataset.	not reported			CodeBERT	To avoid data leakage, we removed the duplicated patches from the training and validation set. Particularly, we removed a patch if it is syntactically equivalent to a patch in the evaluation set. 	We consider 666 patches from Wang et al.’s dataset and 223 developer’s patches from DEFECTS4J [14] as the training and validation set and 139 patches from Xiong et al. [66] as evaluation set following previous work [55], [55], [66], [71]. Note that, there may be duplication between Wang et al. ’s dataset, Defects4J’s patches, and Xiong et al.’s dataset. To avoid data leakage, we removed the duplicated patches from the training and validation set. Particularly, we removed a patch if it is syntactically equivalent to a patch in the evaluation set. As a result, we obtained 746 (out of 889) patches for the training and validation phase. This included 347 correct patches and 399 overfitting patches. We use 90% of these patches (90% × 746 = 671 patches) for training our learning model and the remaining 75 patches for validation.		CodeBERT --	TRUE	TRUE	TRUE	CodeBERT	TRUE	TRUE	cited vaswani	not reported	not reported	FALSE			FALSE	n/a	patch and test suite; an APR-generated patch, the original buggy program, and its correct (ground truth) version		FALSE	FALSE	uses test suite and original program as approximations for correctness (inputs to tool)		
TSE	Nguyen, Truong Giang, et al	Multi-Granularity Detector for Vulnerability Fixes.	2023	https://ieeexplore-ieee-org.ezproxy1.lib.asu.edu/stamp/stamp.jsp?tp=&arnumber=10138621	119	Nguyen, Truong Giang, et al. "Multi-Granularity Detector for Vulnerability Fixes." IEEE Transactions on Software Engineering (2023).	vulnerability-fixing commit classification; machine learning; deep learning; software security;		TRUE	FALSE	FALSE	FALSE	TRUE	Pemma		THIS PAPER IS FOCUSED ON TRYING TO IDENTIFY RELEVANT COMMITS FOR VULNERABILITY FIXES!! SKIPPING!!  "We observe that noise can occur at different levels of detail, making it challenging to detect vulnerability fixes accurately. To address these challenges and boost the effectiveness of prior works, we propose MiDas (Multi-Granularity Detector for Vulnerability Fixes). Unique from prior works, MiDas constructs different neural networks for each level of code change granularity, corresponding to commit-level, file-level, hunk-level, and line-level, following their natural organi- zation and then use an ensemble model combining all base models to output the final prediction. This design allows MiDas to better cope with the noisy and highly-imbalanced nature of vulnerability-fixing commit data. "	FALSE	TRUE	FALSE	FALSE	FALSE	FALSE	FALSE																FALSE	FALSE	FALSE		FALSE	FALSE				FALSE			FALSE				FALSE	FALSE			